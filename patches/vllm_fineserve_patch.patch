diff --git a/patches/0001-omniserve-backed-EngineCore-enabled.patch b/patches/0001-omniserve-backed-EngineCore-enabled.patch
new file mode 100644
index 000000000..5605e05d3
--- /dev/null
+++ b/patches/0001-omniserve-backed-EngineCore-enabled.patch
@@ -0,0 +1,1278 @@
+From e35e5179119980748b7c4ad2aaf55b5c1dc38595 Mon Sep 17 00:00:00 2001
+From: "jm2.son" <jm2.son@samsung.com>
+Date: Wed, 23 Jul 2025 01:51:39 +0000
+Subject: [PATCH] omniserve-backed EngineCore enabled
+
+---
+ vllm/config.py                |    4 +-
+ vllm/v1/engine/core_client.py |   23 +-
+ vllm/v1/engine/core_qoq.py    | 1180 +++++++++++++++++++++++++++++++++
+ 3 files changed, 1203 insertions(+), 4 deletions(-)
+ create mode 100644 vllm/v1/engine/core_qoq.py
+
+diff --git a/vllm/config.py b/vllm/config.py
+index 3fbb6015f..a4ba99887 100644
+--- a/vllm/config.py
++++ b/vllm/config.py
+@@ -851,7 +851,7 @@ class ModelConfig:
+         return quant_cfg
+ 
+     def _verify_quantization(self) -> None:
+-        supported_quantization = QUANTIZATION_METHODS
++        supported_quantization = QUANTIZATION_METHODS + ['qoq']
+         optimized_quantization_methods = [
+             "fp8", "marlin", "modelopt", "gptq_marlin_24", "gptq_marlin",
+             "awq_marlin", "fbgemm_fp8", "compressed-tensors", "experts_int8",
+@@ -4345,7 +4345,7 @@ class VllmConfig:
+             load_config: LoadConfig) -> Optional[QuantizationConfig]:
+         """Get the quantization config."""
+         from vllm.platforms import current_platform
+-        if model_config.quantization is not None:
++        if model_config.quantization is not None and model_config.quantization.lower() != 'qoq':
+             from vllm.model_executor.model_loader.weight_utils import (
+                 get_quant_config)
+             quant_config = get_quant_config(model_config, load_config)
+diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py
+index 7eff377b7..79b404f67 100644
+--- a/vllm/v1/engine/core_client.py
++++ b/vllm/v1/engine/core_client.py
+@@ -40,7 +40,6 @@ AnyFuture = Union[asyncio.Future[Any], Future[Any]]
+ 
+ _R = TypeVar('_R')  # Return type for collective_rpc
+ 
+-
+ class EngineCoreClient(ABC):
+     """
+     EngineCoreClient: subclasses handle different methods for pushing 
+@@ -220,6 +219,17 @@ class InprocClient(EngineCoreClient):
+     """
+ 
+     def __init__(self, *args, **kwargs):
++        
++        if len(args) > 0 and isinstance(args[0], VllmConfig):
++            vllm_config = args[0]
++            if (vllm_config.model_config.quantization is not None and 
++                vllm_config.model_config.quantization.lower() == 'qoq'):
++            
++                from vllm.v1.engine.core_qoq import QOQEngineCoreWrapper
++                self.engine_core = QOQEngineCoreWrapper(*args, **kwargs)
++                
++                return
++        
+         self.engine_core = EngineCore(*args, **kwargs)
+ 
+     def get_output(self) -> EngineCoreOutputs:
+@@ -469,6 +479,14 @@ class MPClient(EngineCoreClient):
+         handshake_address = get_engine_client_zmq_addr(
+             local_only, host, parallel_config.data_parallel_rpc_port)
+ 
++        run_engine_core_callback_fcn = EngineCoreProc.run_engine_core
++        
++        if (vllm_config.model_config.quantization is not None and 
++            vllm_config.model_config.quantization.lower() == 'qoq'):
++        
++            from vllm.v1.engine.core_qoq import QOQEngineCoreProc
++            run_engine_core_callback_fcn = QOQEngineCoreProc.run_engine_core
++
+         with zmq_socket_ctx(handshake_address, zmq.ROUTER,
+                             bind=True) as handshake_socket:
+ 
+@@ -477,7 +495,8 @@ class MPClient(EngineCoreClient):
+                 # In server mode, start_index and local_start_index will
+                 # both be 0.
+                 self.resources.engine_manager = CoreEngineProcManager(
+-                    EngineCoreProc.run_engine_core,
++                    #EngineCoreProc.run_engine_core,
++                    run_engine_core_callback_fcn,
+                     vllm_config=vllm_config,
+                     executor_class=executor_class,
+                     log_stats=log_stats,
+diff --git a/vllm/v1/engine/core_qoq.py b/vllm/v1/engine/core_qoq.py
+new file mode 100644
+index 000000000..117da16c4
+--- /dev/null
++++ b/vllm/v1/engine/core_qoq.py
+@@ -0,0 +1,1180 @@
++# SPDX-License-Identifier: Apache-2.0
++# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
++import os
++import queue
++import signal
++import sys
++import threading
++import time
++from collections import deque
++from collections.abc import Generator
++from concurrent.futures import Future
++from contextlib import ExitStack, contextmanager
++from inspect import isclass, signature
++from logging import DEBUG
++from typing import Any, Callable, Optional, TypeVar, Union, Iterable, List
++
++import msgspec
++import zmq
++from collections import defaultdict, deque
++
++from vllm.config import ParallelConfig, VllmConfig
++from vllm.distributed import stateless_destroy_torch_distributed_process_group
++from vllm.executor.multiproc_worker_utils import _add_prefix
++from vllm.logger import init_logger
++from vllm.logging_utils.dump_input import dump_engine_exception
++from vllm.lora.request import LoRARequest
++from vllm.transformers_utils.config import (
++    maybe_register_config_serialize_by_value)
++from vllm.utils import make_zmq_socket, resolve_obj_by_qualname
++from vllm.v1.core.kv_cache_utils import (get_kv_cache_config,
++                                         unify_kv_cache_configs)
++from vllm.v1.core.sched.interface import SchedulerInterface
++from vllm.v1.core.sched.output import SchedulerOutput
++from vllm.v1.core.sched.scheduler import Scheduler as V1Scheduler
++from vllm.v1.engine import (EngineCoreOutput, EngineCoreOutputs, EngineCoreRequest,
++                            EngineCoreRequestType, UtilityOutput)
++from vllm.v1.engine.mm_input_cache import MirroredProcessingCache
++from vllm.v1.executor.abstract import Executor
++from vllm.v1.kv_cache_interface import KVCacheConfig
++from vllm.v1.metrics.stats import SchedulerStats
++from vllm.v1.outputs import ModelRunnerOutput
++from vllm.v1.request import Request, RequestStatus
++from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
++from vllm.v1.structured_output import StructuredOutputManager
++from vllm.v1.utils import EngineHandshakeMetadata, EngineZmqAddresses
++from vllm.version import __version__ as VLLM_VERSION
++
++logger = init_logger(__name__)
++
++POLLING_TIMEOUT_S = 2.5
++HANDSHAKE_TIMEOUT_MINS = 5
++
++_R = TypeVar('_R')  # Return type for collective_rpc
++
++## Omniserve import
++from omniserve.engine.arg_utils import EngineArgs as QOQEngineArgs
++from omniserve.sampling_params import SamplingParams as QOQSamplingParams
++from omniserve.sequence import (
++    SamplerOutput,
++    Sequence,
++    SequenceGroup,
++    SequenceGroupOutput,
++    SequenceStatus as QOQSequenceStatus,
++)
++from omniserve.engine.llm_engine import LLMEngine as QOQLLMEngine
++##
++
++from vllm.v1.engine import FinishReason
++
++def get_qoq_engine_args(vllm_config:VllmConfig):
++    engine_args = QOQEngineArgs(vllm_config.model_config.model)
++    
++    # model config
++    vllm_model_config = vllm_config.model_config
++    
++    engine_args.tokenizer = vllm_model_config.tokenizer if vllm_model_config.tokenizer is not None else model
++    engine_args.tokenizer_mode = vllm_model_config.tokenizer_mode
++    engine_args.trust_remote_code = vllm_model_config.trust_remote_code
++    
++    engine_args.download_dir = vllm_config.load_config.download_dir
++    engine_args.load_format = "auto"
++    engine_args.dtype = vllm_model_config.dtype
++    engine_args.seed = vllm_model_config.seed
++    engine_args.quantization = None 
++    
++    # cache config
++    vllm_cache_confing = vllm_config.cache_config
++    
++    #engine_args.block_size = vllm_cache_confing.block_size  ### Omniserve kernel은 현재 block_size 64만 지원함.
++    engine_args.gpu_memory_utilization = vllm_cache_confing.gpu_memory_utilization
++    engine_args.swap_space = vllm_cache_confing.swap_space
++    #cache_dtype = vllm_cache_confing.cache_dtype
++    #cache_bits = 4 # MAGIC NUMBER FOR int4 kvcache
++    
++    # scheduler config
++    vllm_scheduler_config = vllm_config.scheduler_config
++    
++    engine_args.max_num_batched_tokens = vllm_scheduler_config.max_num_batched_tokens
++    # default 262144
++    
++    engine_args.max_num_seqs = vllm_scheduler_config.max_num_seqs
++    # default 256
++    
++    engine_args.max_model_len = vllm_scheduler_config.max_model_len
++    
++    
++    # parallel config
++    vllm_parallel_config = vllm_config.parallel_config
++    engine_args.pipeline_parallel_size = vllm_parallel_config.pipeline_parallel_size
++    engine_args.tensor_parallel_size = vllm_parallel_config.tensor_parallel_size
++    
++    
++    # device & ifb setup
++    ## precision, kv-quant-granularity
++    engine_args.kv_quant_granularity = "fine_grained"
++    engine_args.precision = "w4a8kv4"
++    engine_args.group_size = 128
++    engine_args.ifb_mode = True
++    
++    engine_args.sparse_decode_mode = 0
++    
++    ## quant_path
++    engine_args.quant_path = engine_args.model
++    
++    engine_args.chunk_prefill_size = 1024000
++    engine_args.max_num_batched_tokens = 4195000
++
++    return engine_args
++    
++    
++
++class QOQEngineCoreWrapper:
++    """Inner loop of vLLM's Engine."""
++
++    # JMSON TODO
++    def __init__(self,
++                 vllm_config: VllmConfig,
++                 executor_class: type[Executor],
++                 log_stats: bool,
++                 executor_fail_callback: Optional[Callable] = None):
++        assert vllm_config.model_config.runner_type != "pooling"
++        
++        qengine_args = get_qoq_engine_args(vllm_config)
++        self._engine = QOQLLMEngine.from_engine_args(qengine_args)
++        
++        # plugins need to be loaded at the engine/scheduler level too
++        
++        from vllm.plugins import load_general_plugins
++        load_general_plugins()
++
++        self.vllm_config = vllm_config
++        logger.info("Initializing a V1 LLM engine (v%s) with config: %s",
++                    VLLM_VERSION, vllm_config)
++
++
++        # Setup Model.
++        # self.model_executor = executor_class(vllm_config)
++        # if executor_fail_callback is not None:
++        #     self.model_executor.register_failure_callback(
++        #         executor_fail_callback)
++        self.model_executor = None
++
++        # Setup KV Caches and update CacheConfig after profiling.
++        # num_gpu_blocks, num_cpu_blocks, kv_cache_config = \
++        #     self._initialize_kv_caches(vllm_config)
++
++        vllm_config.cache_config.num_gpu_blocks = self._engine.cache_config.num_retrieval_gpu_blocks
++        vllm_config.cache_config.num_cpu_blocks = self._engine.cache_config.num_retrieval_cpu_blocks
++
++        self.structured_output_manager = StructuredOutputManager(vllm_config)
++
++        # Setup MM Input Mapper.
++        self.mm_input_cache_server = MirroredProcessingCache(
++            vllm_config.model_config)
++
++        # Setup batch queue for pipeline parallelism.
++        # Batch queue for scheduled batches. This enables us to asynchronously
++        # schedule and execute batches, and is required by pipeline parallelism
++        # to eliminate pipeline bubbles.
++        # self.batch_queue_size = self.model_executor.max_concurrent_batches
++        # self.batch_queue: Optional[queue.Queue[tuple[Future[ModelRunnerOutput],
++        #                                              SchedulerOutput]]] = None
++        # if self.batch_queue_size > 1:
++        #     logger.info("Batch queue is enabled with size %d",
++        #                 self.batch_queue_size)
++        #     self.batch_queue = queue.Queue(self.batch_queue_size)
++        
++        self.finish_reason_mapper = {
++            QOQSequenceStatus.FINISHED_STOPPED: FinishReason.STOP,
++            QOQSequenceStatus.FINISHED_LENGTH_CAPPED: FinishReason.LENGTH,
++            QOQSequenceStatus.FINISHED_ABORTED: FinishReason.ABORT,
++        }
++
++    # def _initialize_kv_caches(
++    #         self, vllm_config: VllmConfig) -> tuple[int, int, KVCacheConfig]:
++    #     start = time.time()
++
++    #     # Get all kv cache needed by the model
++    #     kv_cache_specs = self.model_executor.get_kv_cache_specs()
++
++    #     # Profiles the peak memory usage of the model to determine how much
++    #     # memory can be allocated for kv cache.
++    #     available_gpu_memory = self.model_executor.determine_available_memory()
++
++    #     assert len(kv_cache_specs) == len(available_gpu_memory)
++    #     # Get the kv cache tensor size
++    #     kv_cache_configs = [
++    #         get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
++    #                             available_gpu_memory_one_worker)
++    #         for kv_cache_spec_one_worker, available_gpu_memory_one_worker in
++    #         zip(kv_cache_specs, available_gpu_memory)
++    #     ]
++
++    #     # Since we use a shared centralized controller, we need the
++    #     # `kv_cache_config` to be consistent across all workers to make sure
++    #     # all the memory operators can be applied to all workers.
++    #     unify_kv_cache_configs(kv_cache_configs)
++
++    #     # All workers have the same kv_cache_config except layer names, so use
++    #     # an arbitrary one to initialize the scheduler.
++    #     assert all([
++    #         cfg.num_blocks == kv_cache_configs[0].num_blocks
++    #         for cfg in kv_cache_configs
++    #     ])
++    #     num_gpu_blocks = kv_cache_configs[0].num_blocks
++    #     num_cpu_blocks = 0
++    #     scheduler_kv_cache_config = kv_cache_configs[0]
++
++    #     # Initialize kv cache and warmup the execution
++    #     self.model_executor.initialize_from_config(kv_cache_configs)
++
++    #     elapsed = time.time() - start
++    #     logger.info(("init engine (profile, create kv cache, "
++    #                  "warmup model) took %.2f seconds"), elapsed)
++    #     return num_gpu_blocks, num_cpu_blocks, scheduler_kv_cache_config
++
++    # JMSON TODO
++    def add_request(self, request: EngineCoreRequest):
++        """Add request to the scheduler."""
++
++        # if request.mm_hashes is not None:
++        #     # Here, if hash exists for a multimodal input, then it will be
++        #     # fetched from the cache, else it will be added to the cache.
++        #     # Note that the cache here is mirrored with the client cache, so
++        #     # anything that has a hash must have a HIT cache entry here
++        #     # as well.
++        #     assert request.mm_inputs is not None
++        #     request.mm_inputs = self.mm_input_cache_server.get_and_update_p1(
++        #         request.mm_inputs, request.mm_hashes)
++
++        # req = Request.from_engine_core_request(request)
++        # if req.use_structured_output:
++        #     # Start grammar compilation asynchronously
++        #     self.structured_output_manager.grammar_init(req)
++
++        # if req.kv_transfer_params is not None and (
++        #         not self.scheduler.get_kv_connector()):
++        #     logger.warning("Got kv_transfer_params, but no KVConnector found. "
++        #                    "Disabling KVTransfer for this request.")
++
++        # self.scheduler.add_request(req)
++        
++        # request mapper from vLLM to QoQ
++        
++        request_id = request.request_id
++        prompt = ""
++        prompt_token_ids = request.prompt_token_ids
++        
++        sampling_params = request.sampling_params
++        
++        #sampling_params.best_of = sampling_params.n if sampling_params.best_of is None else sampling_params.best_of
++        
++        # vllm_sp = request.sampling_params
++        # sampling_params = QOQSamplingParams(
++        #     n=vllm_sp.n,
++        #     best_of=vllm_sp.best_of,
++        #     presence_penalty=vllm_sp.presence_penalty,
++        #     frequency_penalty=vllm_sp.frequency_penalty,
++        #     repetition_penalty=vllm_sp.repetition_penalty,
++        #     temperature=vllm_sp.temperature,
++        #     top_p=vllm_sp.top_p,
++        #     top_k=-1 if vllm_sp.top_k < 1 else vllm_sp.top_k,
++        #     min_p=vllm_sp.min_p,
++        #     stop=vllm_sp.stop,
++        #     stop_token_ids=vllm_sp.stop_token_ids,
++        #     include_stop_str_in_output=vllm_sp.include_stop_str_in_output,
++        #     ignore_eos=vllm_sp.ignore_eos,
++        #     max_tokens=vllm_sp.max_tokens,
++        #     logprobs=vllm_sp.logprobs,
++        #     prompt_logprobs=vllm_sp.prompt_logprobs,
++        #     skip_special_tokens=vllm_sp.skip_special_tokens,
++        #     spaces_between_special_tokens=vllm_sp.spaces_between_special_tokens,
++        #     logits_processors=vllm_sp.logits_processors,
++        # )
++        
++        self._engine.add_request(
++            request_id,
++            prompt,
++            sampling_params,
++            prompt_token_ids=prompt_token_ids
++            )
++        
++
++    def abort_requests(self, request_ids: list[str]):
++        """Abort requests from the scheduler."""
++
++        self._engine.abort_request(request_ids)
++
++    # JMSON TODO
++    def execute_model(self, scheduler_output: SchedulerOutput):
++        try:
++            return self.model_executor.execute_model(scheduler_output)
++        except BaseException as err:
++            # NOTE: This method is exception-free
++            dump_engine_exception(self.vllm_config, scheduler_output,
++                                  self.scheduler.make_stats())
++            # Re-raise exception
++            raise err
++
++    def qoq_process_output(self, output):
++        
++        scheduler_outputs = self._engine.scheduler_outputs
++        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups
++        _outputs: dict[int, list[EngineCoreOutput]] = defaultdict(list)
++        
++        for seq_group, outputs in zip(scheduled_seq_groups, output):
++            self._engine._process_sequence_group_outputs(seq_group, outputs)
++        
++        request_outputs: List = []
++        num_finished = 0
++        
++        for seq_group in scheduled_seq_groups:
++            req_id = seq_group.request_id
++            
++            seqs = seq_group.get_seqs()
++            
++            # TODO finished 효과를 고려하여,
++            assert len(seqs) == 1
++            # if seq_group.is_finished():
++            #     # free kv cache
++            #     self._engine.scheduler.free_seq(seqs[0])
++            #     request_outputs.append(
++            #         {
++            #             "id": seqs[0].seq_id,
++            #             "text": seqs[0].output_text,
++            #             "finished": True,
++            #         }
++            #     )
++            #     num_finished += 1
++            # else:
++            #     request_outputs.append(
++            #         {
++            #             "id": seqs[0].seq_id,
++            #             "tokens": seqs[0].get_token_ids(),
++            #             "finished": False,
++            #         }
++            #     )
++            
++            ## finish reason mapper
++            
++            client_idx = 0 # TODO
++            if seq_group.is_finished():
++                _outputs[client_idx].append(
++                    EngineCoreOutput(
++                        request_id=req_id,
++                        new_token_ids=[seqs[0].get_last_token_id()],
++                        finish_reason=self.finish_reason_mapper.get(seqs[0].status, None),
++                    )
++                )
++                num_finished += 1
++                self._engine.scheduler.free_seq(seqs[0])
++            else:
++                _outputs[client_idx].append(
++                    EngineCoreOutput(
++                        request_id=req_id,
++                        new_token_ids=[seqs[0].get_last_token_id()],
++                    )
++                )
++            
++        ###
++        # outputs[request.client_index].append(
++        #             EngineCoreOutput(
++        #                 request_id=req_id,
++        #                 new_token_ids=new_token_ids,
++        #                 finish_reason=request.get_finished_reason(),
++        #                 new_logprobs=new_logprobs,
++        #                 new_prompt_logprobs_tensors=prompt_logprobs_tensors,
++        #                 stop_reason=request.stop_reason,
++        #                 events=request.take_events(),
++        #                 kv_transfer_params=kv_transfer_params,
++        #                 num_cached_tokens=request.num_cached_tokens,
++        #             ))
++                
++        engine_core_outputs = {
++            client_index: EngineCoreOutputs(outputs=outs)
++            for client_index, outs in _outputs.items()
++        }
++        
++        ### self.client_index req_id별로 저장해둬야 할듯.
++
++        # Free the finished sequence groups.
++        self._engine.scheduler.free_finished_seq_groups()
++
++        return engine_core_outputs
++
++    # JMSON TODO
++    def step(self) -> tuple[dict[int, EngineCoreOutputs], bool]:
++        """Schedule, execute, and make output.
++
++        Returns tuple of outputs and a flag indicating whether the model
++        was executed.
++        """
++
++        # Check for any requests remaining in the scheduler - unfinished,
++        # or finished and not yet removed from the batch.
++        # if not self.scheduler.has_requests():
++        #     return {}, False
++        # scheduler_output = self.scheduler.schedule()
++        
++        # model_output = self.execute_model(scheduler_output)
++        
++        # engine_core_outputs = self.scheduler.update_from_output(
++        #     scheduler_output, model_output)  # type: ignore
++
++        # return (engine_core_outputs,
++        #         scheduler_output.total_num_scheduled_tokens > 0)
++
++        output = self._engine.step() # guesss it is qoq SamplerOutput
++        
++        engine_core_outputs = self.qoq_process_output(output)
++        
++        return (engine_core_outputs, 
++                len(self._engine.scheduler.has_unfinished_seqs()) > 0 )
++        
++    # def step_with_batch_queue(
++    #         self) -> tuple[Optional[dict[int, EngineCoreOutputs]], bool]:
++    #     """Schedule and execute batches with the batch queue.
++    #     Note that if nothing to output in this step, None is returned.
++
++    #     The execution flow is as follows:
++    #     1. Try to schedule a new batch if the batch queue is not full.
++    #     If a new batch is scheduled, directly return an empty engine core
++    #     output. In other words, fulfilling the batch queue has a higher priority
++    #     than getting model outputs.
++    #     2. If there is no new scheduled batch, meaning that the batch queue
++    #     is full or no other requests can be scheduled, we block until the first
++    #     batch in the job queue is finished.
++    #     3. Update the scheduler from the output.
++    #     """
++    #     assert self.batch_queue is not None
++
++    #     engine_core_outputs = None
++    #     scheduler_output = None
++    #     # Try to schedule a new batch if the batch queue is not full, but
++    #     # the scheduler may return an empty batch if all requests are scheduled.
++    #     # Note that this is not blocking.
++    #     if not self.batch_queue.full():
++    #         scheduler_output = self.scheduler.schedule()
++    #         if scheduler_output.total_num_scheduled_tokens > 0:
++    #             future = self.model_executor.execute_model(scheduler_output)
++    #             self.batch_queue.put_nowait(
++    #                 (future, scheduler_output))  # type: ignore
++
++    #     scheduled_batch = (scheduler_output is not None
++    #                        and scheduler_output.total_num_scheduled_tokens > 0)
++
++    #     # If no more requests can be scheduled and the job queue is not empty,
++    #     # block until the first batch in the job queue is finished.
++    #     # TODO(comaniac): Ideally we should peek the first batch in the
++    #     # job queue to check if it's finished before scheduling a new batch,
++    #     # but peeking the first element in a queue is not thread-safe,
++    #     # so we need more work.
++    #     if not scheduled_batch and not self.batch_queue.empty():
++    #         future, scheduler_output = self.batch_queue.get_nowait()
++    #         # Blocking until the first result is available.
++    #         model_output = future.result()
++    #         self.batch_queue.task_done()
++    #         engine_core_outputs = (self.scheduler.update_from_output(
++    #             scheduler_output, model_output))
++
++    #     return engine_co   re_outputs, scheduled_batch
++
++    def shutdown(self):
++        # self.structured_output_manager.clear_backend()
++        # if self.model_executor:
++        #     self.model_executor.shutdown()
++        # if self.scheduler:
++        #     self.scheduler.shutdown()
++        pass
++
++    def profile(self, is_start: bool = True):
++        #self.model_executor.profile(is_start)
++        pass
++
++    # JMSON TODO
++    def reset_mm_cache(self):
++        # # NOTE: Since this is mainly for debugging, we don't attempt to
++        # # re-sync the internal caches (P0 processor, P0 mirror, P1 mirror)
++        # if self.scheduler.has_unfinished_requests():
++        #     logger.warning("Resetting the multi-modal cache when requests are "
++        #                    "in progress may lead to desynced internal caches.")
++
++        self.mm_input_cache_server.reset()
++
++    def reset_prefix_cache(self):
++        # self.scheduler.reset_prefix_cache()
++        pass
++
++    def sleep(self, level: int = 1):
++        # self.model_executor.sleep(level)
++        pass
++
++    def wake_up(self, tags: Optional[list[str]] = None):
++        # self.model_executor.wake_up(tags)
++        pass
++
++    def is_sleeping(self) -> bool:
++        # return self.model_executor.is_sleeping
++        return False
++
++    def execute_dummy_batch(self):
++        # self.model_executor.collective_rpc("execute_dummy_batch")
++        pass
++
++    def add_lora(self, lora_request: LoRARequest) -> bool:
++        raise NotImplementedError
++
++    def remove_lora(self, lora_id: int) -> bool:
++        raise NotImplementedError
++
++    def list_loras(self) -> set[int]:
++        raise NotImplementedError
++
++    def pin_lora(self, lora_id: int) -> bool:
++        raise NotImplementedError
++
++
++    def save_sharded_state(
++        self,
++        path: str,
++        pattern: Optional[str] = None,
++        max_size: Optional[int] = None,
++    ) -> None:
++    
++        # self.model_executor.save_sharded_state(path=path,
++        #                                        pattern=pattern,
++        #                                        max_size=max_size)
++        pass
++
++    # JMSON TODO
++    def collective_rpc(self,
++                       method: Union[str, Callable[..., _R]],
++                       timeout: Optional[float] = None,
++                       args: tuple = (),
++                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:
++        # return self.model_executor.collective_rpc(method, timeout, args,
++        #                                           kwargs)
++        return self._engine._run_workers(method, *args, **kwargs)
++        
++        
++    def save_tensorized_model(
++        self,
++        tensorizer_config,
++    ) -> None:
++        
++        # self.model_executor.save_tensorized_model(
++        #     tensorizer_config=tensorizer_config, )
++        raise NotImplementedError
++
++class QOQEngineCoreProc(QOQEngineCoreWrapper):
++    """ZMQ-wrapper for running EngineCore in background process."""
++
++    ENGINE_CORE_DEAD = b'ENGINE_CORE_DEAD'
++
++    def __init__(
++        self,
++        vllm_config: VllmConfig,
++        on_head_node: bool,
++        handshake_address: str,
++        executor_class: type[Executor],
++        log_stats: bool,
++        engine_index: int = 0,
++    ):
++        self.input_queue = queue.Queue[tuple[EngineCoreRequestType, Any]]()
++        self.output_queue = queue.Queue[Union[tuple[int, EngineCoreOutputs],
++                                              bytes]]()
++        executor_fail_callback = lambda: self.input_queue.put_nowait(
++            (EngineCoreRequestType.EXECUTOR_FAILED, b''))
++
++        self.engine_index = engine_index
++        identity = self.engine_index.to_bytes(length=2, byteorder="little")
++        self.engines_running = False
++
++        with self._perform_handshake(handshake_address, identity, on_head_node,
++                                     vllm_config) as addresses:
++            self.client_count = len(addresses.outputs)
++
++            # Set up data parallel environment.
++            self.has_coordinator = addresses.coordinator_output is not None
++            self._init_data_parallel(vllm_config)
++
++            super().__init__(vllm_config, executor_class, log_stats,
++                             executor_fail_callback)
++
++        self.step_fn = self.step
++
++        # Background Threads and Queues for IO. These enable us to
++        # overlap ZMQ socket IO with GPU since they release the GIL,
++        # and to overlap some serialization/deserialization with the
++        # model forward pass.
++        # Threads handle Socket <-> Queues and core_busy_loop uses Queue.
++        threading.Thread(target=self.process_input_sockets,
++                         args=(addresses.inputs, addresses.coordinator_input,
++                               identity),
++                         daemon=True).start()
++        self.output_thread = threading.Thread(
++            target=self.process_output_sockets,
++            args=(addresses.outputs, addresses.coordinator_output,
++                  self.engine_index),
++            daemon=True)
++        self.output_thread.start()
++
++    @contextmanager
++    def _perform_handshake(
++            self, handshake_address: str, identity: bytes, on_head_node: bool,
++            vllm_config: VllmConfig
++    ) -> Generator[EngineZmqAddresses, None, None]:
++        input_ctx = zmq.Context()
++        with make_zmq_socket(input_ctx,
++                             handshake_address,
++                             zmq.DEALER,
++                             identity=identity,
++                             linger=5000,
++                             bind=False) as handshake_socket:
++            # Register engine with front-end.
++            addresses = self.startup_handshake(handshake_socket, on_head_node,
++                                               vllm_config.parallel_config)
++
++            # Update config which may have changed from the handshake
++            vllm_config.__post_init__()
++
++            yield addresses
++
++            # Send ready message.
++            num_gpu_blocks = vllm_config.cache_config.num_gpu_blocks
++            handshake_socket.send(
++                msgspec.msgpack.encode({
++                    "status": "READY",
++                    "local": on_head_node,
++                    "num_gpu_blocks": num_gpu_blocks,
++                }))
++
++    @staticmethod
++    def startup_handshake(
++            handshake_socket: zmq.Socket, on_head_node: bool,
++            parallel_config: ParallelConfig) -> EngineZmqAddresses:
++
++        # Send registration message.
++        handshake_socket.send(
++            msgspec.msgpack.encode({
++                "status": "HELLO",
++                "local": on_head_node,
++            }))
++
++        # Receive initialization message.
++        logger.info("Waiting for init message from front-end.")
++        if not handshake_socket.poll(timeout=HANDSHAKE_TIMEOUT_MINS * 60_000):
++            raise RuntimeError("Did not receive response from front-end "
++                               f"process within {HANDSHAKE_TIMEOUT_MINS} "
++                               f"minutes")
++        init_bytes = handshake_socket.recv()
++        init_message: EngineHandshakeMetadata = msgspec.msgpack.decode(
++            init_bytes, type=EngineHandshakeMetadata)
++        logger.debug("Received init message: %s", init_message)
++
++        received_parallel_config = init_message.parallel_config
++        for key, value in received_parallel_config.items():
++            setattr(parallel_config, key, value)
++
++        return init_message.addresses
++
++    # JMSON TODO
++    @staticmethod
++    def run_engine_core(*args,
++                        dp_rank: int = 0,
++                        local_dp_rank: int = 0,
++                        **kwargs):
++        """Launch EngineCore busy loop in background process."""
++
++        # Signal handler used for graceful termination.
++        # SystemExit exception is only raised once to allow this and worker
++        # processes to terminate without error
++        shutdown_requested = False
++
++        # Ensure we can serialize transformer config after spawning
++        maybe_register_config_serialize_by_value()
++
++        def signal_handler(signum, frame):
++            nonlocal shutdown_requested
++            if not shutdown_requested:
++                shutdown_requested = True
++                raise SystemExit()
++
++        # Either SIGTERM or SIGINT will terminate the engine_core
++        signal.signal(signal.SIGTERM, signal_handler)
++        signal.signal(signal.SIGINT, signal_handler)
++
++        engine_core: Optional[QOQEngineCoreWrapper] = None
++        try:
++            parallel_config: ParallelConfig = kwargs[
++                "vllm_config"].parallel_config
++            if parallel_config.data_parallel_size > 1 or dp_rank > 0:
++                # Set data parallel rank for this engine process.
++                
++                # TODO
++                raise NotImplementedError
++            
++                parallel_config.data_parallel_rank = dp_rank
++                parallel_config.data_parallel_rank_local = local_dp_rank
++                engine_core = DPEngineCoreProc(*args, **kwargs)
++            else:
++                engine_core = QOQEngineCoreProc(*args, **kwargs)
++
++            engine_core.run_busy_loop()
++
++        except SystemExit:
++            logger.debug("EngineCore exiting.")
++            raise
++        except Exception as e:
++            if engine_core is None:
++                logger.exception("EngineCore failed to start.")
++            else:
++                logger.exception("EngineCore encountered a fatal error.")
++                engine_core._send_engine_dead()
++            raise e
++        finally:
++            if engine_core is not None:
++                engine_core.shutdown()
++
++    def _init_data_parallel(self, vllm_config: VllmConfig):
++        pass
++
++    # JMSON TODO
++    def run_busy_loop(self):
++        """Core busy loop of the EngineCore."""
++
++        # Loop until process is sent a SIGINT or SIGTERM
++        while True:
++            # 1) Poll the input queue until there is work to do.
++            self._process_input_queue()
++            # 2) Step the engine core and return the outputs.
++            self._process_engine_step()
++
++    # JMSON TODO
++    def _process_input_queue(self):
++        """Exits when an engine step needs to be performed."""
++
++        waited = False
++        while not self.engines_running and not self._engine.has_unfinished_requests():
++            if logger.isEnabledFor(DEBUG) and self.input_queue.empty():
++                logger.debug("EngineCore waiting for work.")
++                waited = True
++            req = self.input_queue.get()
++            self._handle_client_request(*req)
++
++        if waited:
++            logger.debug("EngineCore loop active.")
++
++        # Handle any more client requests.
++        while not self.input_queue.empty():
++            req = self.input_queue.get_nowait()
++            self._handle_client_request(*req)
++
++    def _process_engine_step(self) -> bool:
++        """Called only when there are unfinished local requests."""
++
++        # Step the engine core.
++        outputs, model_executed = self.step_fn()
++        
++        # Put EngineCoreOutputs into the output queue.
++        for output in (outputs.items() if outputs else ()):
++            self.output_queue.put_nowait(output)
++
++        return model_executed
++
++    def _handle_client_request(self, request_type: EngineCoreRequestType,
++                               request: Any) -> None:
++        """Dispatch request from client."""
++
++        if request_type == EngineCoreRequestType.ADD:
++            self.add_request(request)
++        elif request_type == EngineCoreRequestType.ABORT:
++            self.abort_requests(request)
++        elif request_type == EngineCoreRequestType.UTILITY:
++            client_idx, call_id, method_name, args = request
++            output = UtilityOutput(call_id)
++            try:
++                method = getattr(self, method_name)
++                output.result = method(
++                    *self._convert_msgspec_args(method, args))
++            except BaseException as e:
++                logger.exception("Invocation of %s method failed", method_name)
++                output.failure_message = (f"Call to {method_name} method"
++                                          f" failed: {str(e)}")
++            self.output_queue.put_nowait(
++                (client_idx, EngineCoreOutputs(utility_output=output)))
++        elif request_type == EngineCoreRequestType.EXECUTOR_FAILED:
++            raise RuntimeError("Executor failed.")
++        else:
++            logger.error("Unrecognized input request type encountered: %s",
++                         request_type)
++
++    @staticmethod
++    def _convert_msgspec_args(method, args):
++        """If a provided arg type doesn't match corresponding target method
++         arg type, try converting to msgspec object."""
++        if not args:
++            return args
++        arg_types = signature(method).parameters.values()
++        assert len(args) <= len(arg_types)
++        return tuple(
++            msgspec.convert(v, type=p.annotation) if isclass(p.annotation)
++            and issubclass(p.annotation, msgspec.Struct)
++            and not isinstance(v, p.annotation) else v
++            for v, p in zip(args, arg_types))
++
++    def _send_engine_dead(self):
++        """Send EngineDead status to the EngineCoreClient."""
++
++        # Put ENGINE_CORE_DEAD in the queue.
++        self.output_queue.put_nowait(QOQEngineCoreProc.ENGINE_CORE_DEAD)
++
++        # Wait until msg sent by the daemon before shutdown.
++        self.output_thread.join(timeout=5.0)
++        if self.output_thread.is_alive():
++            logger.fatal("vLLM shutdown signal from EngineCore failed "
++                         "to send. Please report this issue.")
++
++    def process_input_sockets(self, input_addresses: list[str],
++                              coord_input_address: Optional[str],
++                              identity: bytes):
++        """Input socket IO thread."""
++
++        # Msgpack serialization decoding.
++        add_request_decoder = MsgpackDecoder(EngineCoreRequest)
++        generic_decoder = MsgpackDecoder()
++
++        with ExitStack() as stack, zmq.Context() as ctx:
++            input_sockets = [
++                stack.enter_context(
++                    make_zmq_socket(ctx,
++                                    input_address,
++                                    zmq.DEALER,
++                                    identity=identity,
++                                    bind=False))
++                for input_address in input_addresses
++            ]
++            if coord_input_address is None:
++                coord_socket = None
++            else:
++                coord_socket = stack.enter_context(
++                    make_zmq_socket(ctx,
++                                    coord_input_address,
++                                    zmq.XSUB,
++                                    identity=identity,
++                                    bind=False))
++                # Send subscription message to coordinator.
++                coord_socket.send(b'\x01')
++
++            # Register sockets with poller.
++            poller = zmq.Poller()
++            for input_socket in input_sockets:
++                # Send initial message to each input socket - this is required
++                # before the front-end ROUTER socket can send input messages
++                # back to us.
++                input_socket.send(b'')
++                poller.register(input_socket, zmq.POLLIN)
++            if coord_socket is not None:
++                poller.register(coord_socket, zmq.POLLIN)
++
++            while True:
++                for input_socket, _ in poller.poll():
++                    # (RequestType, RequestData)
++                    type_frame, *data_frames = input_socket.recv_multipart(
++                        copy=False)
++                    request_type = EngineCoreRequestType(
++                        bytes(type_frame.buffer))
++
++                    # Deserialize the request data.
++                    decoder = add_request_decoder if (
++                        request_type
++                        == EngineCoreRequestType.ADD) else generic_decoder
++                    request = decoder.decode(data_frames)
++
++                    # Push to input queue for core busy loop.
++                    self.input_queue.put_nowait((request_type, request))
++
++    def process_output_sockets(self, output_paths: list[str],
++                               coord_output_path: Optional[str],
++                               engine_index: int):
++        """Output socket IO thread."""
++
++        # Msgpack serialization encoding.
++        encoder = MsgpackEncoder()
++        # Send buffers to reuse.
++        reuse_buffers: list[bytearray] = []
++        # Keep references to outputs and buffers until zmq is finished
++        # with them (outputs may contain tensors/np arrays whose
++        # backing buffers were extracted for zero-copy send).
++        pending = deque[tuple[zmq.MessageTracker, Any, bytearray]]()
++
++        # We must set linger to ensure the ENGINE_CORE_DEAD
++        # message is sent prior to closing the socket.
++        with ExitStack() as stack, zmq.Context() as ctx:
++            sockets = [
++                stack.enter_context(
++                    make_zmq_socket(ctx, output_path, zmq.PUSH, linger=4000))
++                for output_path in output_paths
++            ]
++            coord_socket = stack.enter_context(
++                make_zmq_socket(
++                    ctx, coord_output_path, zmq.PUSH, bind=False,
++                    linger=4000)) if coord_output_path is not None else None
++            max_reuse_bufs = len(sockets) + 1
++
++            while True:
++                output = self.output_queue.get()
++                if output == QOQEngineCoreProc.ENGINE_CORE_DEAD:
++                    for socket in sockets:
++                        socket.send(output)
++                    break
++                assert not isinstance(output, bytes)
++                client_index, outputs = output
++                outputs.engine_index = engine_index
++
++                if client_index == -1:
++                    # Don't reuse buffer for coordinator message
++                    # which will be very small.
++                    assert coord_socket is not None
++                    coord_socket.send_multipart(encoder.encode(outputs))
++                    continue
++
++                # Reclaim buffers that zmq is finished with.
++                while pending and pending[-1][0].done:
++                    reuse_buffers.append(pending.pop()[2])
++
++                buffer = reuse_buffers.pop() if reuse_buffers else bytearray()
++                buffers = encoder.encode_into(outputs, buffer)
++                tracker = sockets[client_index].send_multipart(buffers,
++                                                               copy=False,
++                                                               track=True)
++                if not tracker.done:
++                    ref = outputs if len(buffers) > 1 else None
++                    pending.appendleft((tracker, ref, buffer))
++                elif len(reuse_buffers) < max_reuse_bufs:
++                    # Limit the number of buffers to reuse.
++                    reuse_buffers.append(buffer)
++
++
++class DPEngineCoreProc(QOQEngineCoreProc):
++    """ZMQ-wrapper for running EngineCore in background process
++    in a data parallel context."""
++
++    def __init__(
++        self,
++        vllm_config: VllmConfig,
++        on_head_node: bool,
++        handshake_address: str,
++        executor_class: type[Executor],
++        log_stats: bool,
++    ):
++
++        self._decorate_logs()
++
++        # Counts forward-passes of the model so that we can synchronize
++        # finished with DP peers every N steps.
++        self.counter = 0
++        self.current_wave = 0
++        self.last_counts = (0, 0)
++
++        # Initialize the engine.
++        dp_rank = vllm_config.parallel_config.data_parallel_rank
++        super().__init__(vllm_config, on_head_node, handshake_address,
++                         executor_class, log_stats, dp_rank)
++
++    def _decorate_logs(self):
++        # Add process-specific prefix to stdout and stderr before
++        # we initialize the engine.
++        from multiprocessing import current_process
++        process_name = current_process().name
++        pid = os.getpid()
++        _add_prefix(sys.stdout, process_name, pid)
++        _add_prefix(sys.stderr, process_name, pid)
++
++    def _init_data_parallel(self, vllm_config: VllmConfig):
++
++        # Configure GPUs and stateless process group for data parallel.
++        dp_rank = vllm_config.parallel_config.data_parallel_rank
++        dp_size = vllm_config.parallel_config.data_parallel_size
++        local_dp_rank = vllm_config.parallel_config.data_parallel_rank_local
++
++        assert dp_size > 1
++        assert 0 <= local_dp_rank <= dp_rank < dp_size
++
++        if vllm_config.kv_transfer_config is not None:
++            # modify the engine_id and append the local_dp_rank to it to ensure
++            # that the kv_transfer_config is unique for each DP rank.
++            vllm_config.kv_transfer_config.engine_id = (
++                f"{vllm_config.kv_transfer_config.engine_id}_dp{local_dp_rank}"
++            )
++            logger.debug("Setting kv_transfer_config.engine_id to %s",
++                         vllm_config.kv_transfer_config.engine_id)
++
++        from vllm.platforms import current_platform
++        device_control_env_var = current_platform.device_control_env_var
++        world_size = vllm_config.parallel_config.world_size
++        os.environ[device_control_env_var] = ",".join(
++            str(current_platform.device_id_to_physical_device_id(i))
++            for i in range(local_dp_rank * world_size, (local_dp_rank + 1) *
++                           world_size))
++
++        self.dp_rank = dp_rank
++        self.dp_group = vllm_config.parallel_config.stateless_init_dp_group()
++
++    def shutdown(self):
++        super().shutdown()
++        if dp_group := getattr(self, "dp_group", None):
++            stateless_destroy_torch_distributed_process_group(dp_group)
++
++    def add_request(self, request: EngineCoreRequest):
++        if self.has_coordinator and request.current_wave != self.current_wave:
++            if request.current_wave > self.current_wave:
++                self.current_wave = request.current_wave
++            elif not self.engines_running:
++                # Request received for an already-completed wave, notify
++                # front-end that we need to start the next one.
++                self.output_queue.put_nowait(
++                    (-1, EngineCoreOutputs(start_wave=self.current_wave)))
++
++        super().add_request(request)
++
++    def _handle_client_request(self, request_type: EngineCoreRequestType,
++                               request: Any) -> None:
++        if request_type == EngineCoreRequestType.START_DP_WAVE:
++            new_wave, exclude_eng_index = request
++            if exclude_eng_index != self.engine_index and (
++                    new_wave >= self.current_wave):
++                self.current_wave = new_wave
++                if not self.engines_running:
++                    logger.debug("EngineCore starting idle loop for wave %d.",
++                                 new_wave)
++                    self.engines_running = True
++        else:
++            super()._handle_client_request(request_type, request)
++
++    def _maybe_publish_request_counts(self):
++        if not self.has_coordinator:
++            return
++
++        # Publish our request counts (if they've changed).
++        counts = self.scheduler.get_request_counts()
++        if counts != self.last_counts:
++            self.last_counts = counts
++            stats = SchedulerStats(*counts)
++            self.output_queue.put_nowait(
++                (-1, EngineCoreOutputs(scheduler_stats=stats)))
++
++    def run_busy_loop(self):
++        """Core busy loop of the EngineCore for data parallel case."""
++
++        # Loop until process is sent a SIGINT or SIGTERM
++        while True:
++            # 1) Poll the input queue until there is work to do.
++            self._process_input_queue()
++
++            # 2) Step the engine core.
++            executed = self._process_engine_step()
++            self._maybe_publish_request_counts()
++
++            local_unfinished_reqs = self.scheduler.has_unfinished_requests()
++            if not executed:
++                if not local_unfinished_reqs and not self.engines_running:
++                    # All engines are idle.
++                    continue
++
++                # We are in a running state and so must execute a dummy pass
++                # if the model didn't execute any ready requests.
++                self.execute_dummy_batch()
++
++            # 3) All-reduce operation to determine global unfinished reqs.
++            self.engines_running = self._has_global_unfinished_reqs(
++                local_unfinished_reqs)
++
++            if not self.engines_running:
++                if self.dp_rank == 0:
++                    # Notify client that we are pausing the loop.
++                    logger.debug("Wave %d finished, pausing engine loop.",
++                                 self.current_wave)
++                    self.output_queue.put_nowait(
++                        (-1,
++                         EngineCoreOutputs(wave_complete=self.current_wave)))
++                self.current_wave += 1
++
++    def _has_global_unfinished_reqs(self, local_unfinished: bool) -> bool:
++
++        # Optimization - only perform finish-sync all-reduce every 24 steps.
++        self.counter += 1
++        if self.counter != 24:
++            return True
++        self.counter = 0
++
++        return ParallelConfig.has_unfinished_dp(self.dp_group,
++                                                local_unfinished)
++
++
++class DPEngineCoreActor(DPEngineCoreProc):
++    """
++    Ray actor for running EngineCore in a data parallel context
++    """
++
++    def __init__(
++        self,
++        vllm_config: VllmConfig,
++        on_head_node: bool,
++        addresses: EngineZmqAddresses,
++        executor_class: type[Executor],
++        log_stats: bool,
++        dp_rank: int = 0,
++        local_dp_rank: int = 0,
++    ):
++        self.addresses = addresses
++        vllm_config.parallel_config.data_parallel_rank = dp_rank
++        vllm_config.parallel_config.data_parallel_rank_local = \
++            local_dp_rank
++
++        # Ray sets CUDA_VISIBLE_DEVICES to empty string,
++        # we clean this up to be able to properly initialize
++        # data parallel groups.
++        del os.environ['CUDA_VISIBLE_DEVICES']
++
++        super().__init__(vllm_config, on_head_node, "", executor_class,
++                         log_stats)
++
++    def _decorate_logs(self):
++        pass
++
++    @contextmanager
++    def _perform_handshake(self, handshake_address: str, identity: bytes,
++                           on_head_node: bool, vllm_config: VllmConfig):
++        """
++        For Ray, we don't need to actually perform handshake.
++        All addresses information is known before the actor creation.
++        Therefore, we simply yield these addresses.
++        """
++        yield self.addresses
++
++    def wait_for_init(self):
++        """
++        Wait until the engine core is initialized.
++
++        This is just an empty method. When ray.get() on this method
++        (or any other method of the actor) returns, it is guaranteed
++        that actor creation (i.e., __init__) is complete.
++        """
++        pass
++
++    def run(self):
++        """
++        Run the engine core busy loop.
++        """
++        try:
++            self.run_busy_loop()
++        except SystemExit:
++            logger.debug("EngineCore exiting.")
++            raise
++        except Exception:
++            logger.exception("EngineCore encountered a fatal error.")
++            raise
++        finally:
++            self.shutdown()
+-- 
+2.34.1
+
diff --git a/pyproject.toml b/pyproject.toml
index 307878f7e..ac7a7c3b2 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -176,4 +176,4 @@ root = "./vllm"
 respect-ignore-files = true
 
 [tool.ty.environment]
-python = "./.venv"
+python = "./.venv"
\ No newline at end of file
diff --git a/vllm/config.py b/vllm/config.py
index 3fbb6015f..7b1d19e2b 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -617,8 +617,12 @@ class ModelConfig:
         if (not current_platform.is_neuron() and self.override_neuron_config):
             raise ValueError(
                 "`override_neuron_config` is only supported on Neuron.")
+        if isinstance(self.quantization, str) and self.quantization == 'qoq':
+            pass
+        else:
+            self._verify_quantization()
 
-        self._verify_quantization()
+        # self._verify_quantization()
         self._verify_cuda_graph()
         self._verify_bnb_config()
 
@@ -1504,6 +1508,16 @@ class CacheConfig:
     """This enables dynamic calculation of `k_scale` and `v_scale` when
     kv_cache_dtype is fp8. If `False`, the scales will be loaded from the model
     checkpoint if available. Otherwise, the scales will default to 1.0."""
+    
+    kv_slab_size: int = 2 * 1024 * 1024
+    """KVSlab size of FineServe
+    """
+    kv_slab_host: Optional[str] = "localhost"
+    """KVSlabManager host of FineServe
+    """
+    kv_slab_ports: Optional[list[int]] = field(default_factory=lambda: list(range(5555, 5555+8)))
+    """All KVSlabManager ports of FineServe
+    """
 
     # Will be set after profiling.
     num_gpu_blocks: Optional[int] = field(default=None, init=False)
@@ -4345,7 +4359,7 @@ class VllmConfig:
             load_config: LoadConfig) -> Optional[QuantizationConfig]:
         """Get the quantization config."""
         from vllm.platforms import current_platform
-        if model_config.quantization is not None:
+        if model_config.quantization is not None and model_config.quantization.lower() != 'qoq':
             from vllm.model_executor.model_loader.weight_utils import (
                 get_quant_config)
             quant_config = get_quant_config(model_config, load_config)
@@ -4505,6 +4519,11 @@ class VllmConfig:
                 # Hybrid KV cache manager is not compatible with KV events.
                 self.scheduler_config.disable_hybrid_kv_cache_manager = True
 
+        if self.model_config.quantization == 'qoq':
+            from vllm.v1.engine.core_qoq import QoQConfig
+            self.additional_config = QoQConfig()
+
+
     def update_sizes_for_sequence_parallelism(self,
                                               possible_sizes: list) -> list:
         # remove the sizes that not multiple of tp_size when
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 4ce1b41e4..6ddaf4d4a 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -443,6 +443,12 @@ class EngineArgs:
     enable_multimodal_encoder_data_parallel: bool = \
         ParallelConfig.enable_multimodal_encoder_data_parallel
 
+    # kv slab
+    kv_slab_size: int = CacheConfig.kv_slab_size
+    kv_slab_host: str = CacheConfig.kv_slab_host
+    kv_slab_ports: Optional[list[int]] = get_field(CacheConfig,
+                                                   "kv_slab_ports")
+
     def __post_init__(self):
         # support `EngineArgs(compilation_config={...})`
         # without having to manually construct a
@@ -689,6 +695,13 @@ class EngineArgs:
                                  **cache_kwargs["cpu_offload_gb"])
         cache_group.add_argument("--calculate-kv-scales",
                                  **cache_kwargs["calculate_kv_scales"])
+        # for FineServe
+        cache_group.add_argument("--kv-slab-size",
+                                 **cache_kwargs["kv_slab_size"])
+        cache_group.add_argument("--kv-slab-host",
+                                 **cache_kwargs["kv_slab_host"])
+        cache_group.add_argument("--kv-slab-ports",
+                                 **cache_kwargs["kv_slab_ports"])
 
         # Tokenizer arguments
         tokenizer_kwargs = get_kwargs(TokenizerPoolConfig)
@@ -1065,6 +1078,10 @@ class EngineArgs:
             prefix_caching_hash_algo=self.prefix_caching_hash_algo,
             cpu_offload_gb=self.cpu_offload_gb,
             calculate_kv_scales=self.calculate_kv_scales,
+            use_kv_slab=envs.VLLM_USE_KV_SLAB,
+            kv_slab_size=self.kv_slab_size,
+            kv_slab_host=self.kv_slab_host,
+            kv_slab_ports=self.kv_slab_ports,
         )
 
         # Get the current placement group if Ray is initialized and
diff --git a/vllm/envs.py b/vllm/envs.py
index 80c5f289b..fc66f76ea 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -127,6 +127,7 @@ if TYPE_CHECKING:
     VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS: int = 1
     VLLM_SLEEP_WHEN_IDLE: bool = False
     VLLM_MQ_MAX_CHUNK_BYTES_MB: int = 16
+    VLLM_USE_KV_SLAB: bool = True 
 
 
 def get_default_cache_root():
@@ -870,6 +871,10 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # processes via zmq.
     "VLLM_MQ_MAX_CHUNK_BYTES_MB":
     lambda: int(os.getenv("VLLM_MQ_MAX_CHUNK_BYTES_MB", "16")),
+
+    # Option for using KV Slab in FineServe
+    "VLLM_USE_KV_SLAB":
+    lambda: bool(int(os.getenv("VLLM_USE_KV_SLAB", "0")))
 }
 
 # --8<-- [end:env-vars-definition]
diff --git a/vllm/model_executor/layers/quantization/__init__.py b/vllm/model_executor/layers/quantization/__init__.py
index 1cb23e7a1..71b95c676 100644
--- a/vllm/model_executor/layers/quantization/__init__.py
+++ b/vllm/model_executor/layers/quantization/__init__.py
@@ -35,6 +35,7 @@ QuantizationMethods = Literal[
     "moe_wna16",
     "torchao",
     "auto-round",
+    "qoq" # for supporting qoq in FineServe
 ]
 QUANTIZATION_METHODS: list[str] = list(get_args(QuantizationMethods))
 
@@ -112,6 +113,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
     from .qqq import QQQConfig
     from .torchao import TorchAOConfig
     from .tpu_int8 import Int8TpuConfig
+    from vllm.v1.engine.core_qoq import QoQConfig
 
     method_to_config: dict[str, type[QuantizationConfig]] = {
         "aqlm": AQLMConfig,
@@ -142,6 +144,7 @@ def get_quantization_config(quantization: str) -> type[QuantizationConfig]:
         "moe_wna16": MoeWNA16Config,
         "torchao": TorchAOConfig,
         "auto-round": AutoRoundConfig,
+        "qoq": QoQConfig 
     }
     # Update the `method_to_config` with customized quantization methods.
     method_to_config.update(_CUSTOMIZED_METHOD_TO_QUANT_CONFIG)
diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py
index d21f94727..ec4e54376 100644
--- a/vllm/v1/core/block_pool.py
+++ b/vllm/v1/core/block_pool.py
@@ -16,6 +16,7 @@ from vllm.v1.request import Request
 logger = init_logger(__name__)
 
 
+
 class BlockPool:
     """BlockPool that manages KVCacheBlocks.
     It provides methods to allocate, free and cache the kv cache blocks. The
@@ -347,3 +348,180 @@ class BlockPool:
         events = self.kv_event_queue
         self.kv_event_queue = []
         return events
+
+class ElasticBlockPool:
+    """
+        ElasticBlockPool that manages KVCacheBlocks.
+        It provides same interface as BlockPool, but it manages shared KV cache by communicating with FineServe ResourceManager
+    """
+
+    def __init__(self, num_gpu_blocks: int, tokens_per_block: int, cell_size: int,
+                 num_layers: int, enable_caching: bool, engine_id: int, local_rank: int, host: str, ports: list[int]):
+        assert isinstance(num_gpu_blocks, int) and num_gpu_blocks > 0
+        assert not enable_caching, "Caching is not supported in ElasticBlockPool"
+        self.num_gpu_blocks = num_gpu_blocks
+        # TODO: Support other qformats
+        scale_size = 0 # Assume no quantization or per-tensor quantization
+        self.block_size = tokens_per_block * cell_size + scale_size
+        self.enable_kv_cache_events = False
+        
+        import zmq
+        self.engine_id = engine_id
+        self.local_rank = local_rank
+        self.port = ports[self.local_rank]
+        self.host = host
+        self.ctx = zmq.Context()
+        self.socket = self.ctx.socket(zmq.REQ)
+        self.socket.connect(f"tcp://{self.host}:{self.port}") 
+        
+        from collections import deque
+        self.local_free_blocks: deque[KVCacheBlock] = deque()
+        self.refill_size = 32
+        self.cache_high_watermark = self.refill_size * 2
+        self.return_batch_size = self.refill_size // 2
+        self.last_known_manager_free_blocks = 0
+        self.num_allocated_blocks = 0
+
+        self._sync_num_free_blocks()
+    
+    def shutdown(self):
+        if self._is_shutdown:
+            return
+        
+        if self.local_free_blocks:
+            remaining_blocks = list(self.local_free_blocks)
+            self.local_free_blocks.clear()
+
+            logger.info(f"[Local Rank {self.local_rank} Engine {self.engine_id}] Returning {len(remaining_blocks)} "
+                        f"cached blocks to the manager upon shutdown.")
+            
+            try:
+                self._return_blocks_to_manager(remaining_blocks)
+            except Exception as e:
+                logger.error(f"Engine {self.engine_id}: Failed to return blocks during shutdown: {e}")
+        
+        self._is_shutdown = True
+
+    def __del__(self):
+        self.shutdown()
+
+    def _sync_num_free_blocks(self):
+        try:
+            self.socket.send_pyobj((self.engine_id, "get_num_free_blocks", self.block_size))
+            num_free_blocks = self.socket.recv_pyobj()
+            if isinstance(num_free_blocks, int):
+                self.last_known_manager_free_blocks = num_free_blocks
+        except Exception as e:
+            logger.error(f"Engine {self.engine_id}: Failed to sync free blocks from KVSlabManager: {e}")
+            self.last_known_manager_free_blocks = 0
+
+
+    def get_cached_block(self,
+                         block_hash: BlockHash) -> Optional[KVCacheBlock]:
+        return False
+
+    def cache_full_blocks(
+        self,
+        request: Request,
+        blocks: list[KVCacheBlock],
+        block_hashes: list[BlockHash],
+        num_cached_blocks: int,
+        num_full_blocks: int,
+        block_size: int,
+        hash_fn: Callable,
+    ) -> None:
+        raise NotImplementedError(
+            "Caching is not supported in ElasticBlockPool")
+
+    def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:
+        """Get new blocks from the free block pool.
+
+        Note that we do not check block cache in this function.
+
+        Args:
+            num_blocks: The number of blocks to allocate.
+
+        Returns:
+            A list of new block.
+        """
+        if len(self.local_free_blocks) < num_blocks:
+            num_to_request = max(num_blocks, self.refill_size)
+            self.socket.send_pyobj((self.engine_id, "alloc", (self.block_size, num_to_request)))
+            block_ids, num_free_blocks = self.socket.recv_pyobj()
+
+            if block_ids is None:
+                return None
+            
+            self.last_known_manager_free_blocks = num_free_blocks
+            self.local_free_blocks.extend(block_ids)
+
+        allocated_ids = []
+        for _ in range(num_blocks):
+            allocated_ids.append(self.local_free_blocks.popleft())
+        
+        self.num_allocated_blocks += len(allocated_ids)
+
+        return [KVCacheBlock(bid) for bid in allocated_ids]
+
+    def touch(self, blocks: list[KVCacheBlock]) -> None:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def free_blocks(self, ordered_blocks: Iterable[KVCacheBlock]) -> None:
+        """Free a list of blocks. The blocks should be ordered by their
+        eviction priority, where the first block will be evicted first.
+
+        Args:
+            ordered_blocks: A list of blocks to free ordered by their eviction
+                priority.
+        """
+        freed_block_numbers = [block.block_id for block in ordered_blocks]
+        if not freed_block_numbers:
+            return
+        
+        self.local_free_blocks.extend(freed_block_numbers)
+        self.num_allocated_blocks -= len(freed_block_numbers)
+
+        if len(self.local_free_blocks) > self.cache_high_watermark:
+            blocks_to_return = []
+            num_to_return = min(self.return_batch_size, len(self.local_free_blocks))
+
+            for _ in range(num_to_return):
+                blocks_to_return.append(self.local_free_blocks.popleft())
+            
+            if blocks_to_return:
+                self._return_blocks_to_manager(blocks_to_return)
+
+    def _return_blocks_to_manager(self, block_ids):
+
+        if len(block_ids) > 0:
+            self.socket.send_pyobj((self.engine_id, "free", (self.block_size, block_ids)))
+            manager_free_count = self.socket.recv_pyobj()
+            if isinstance(manager_free_count, int):
+                self.last_known_manager_free_blocks = manager_free_count
+
+    def reset_prefix_cache(self) -> bool:
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+
+    def get_num_free_blocks(self) -> int:
+        """Get the number of free blocks in the pool.
+
+        Returns:
+            The number of free blocks.
+        """
+        
+        return self.last_known_manager_free_blocks + len(self.local_free_blocks)
+
+    def get_usage(self) -> float:
+        """Get the KV cache usage.
+
+        Returns:
+            The KV cache usage (between 0.0 and 1.0).
+        """
+        
+        return (self.num_allocated_blocks / self.num_gpu_blocks)
+    
+    def take_events(self) -> list[KVCacheEvent]:
+        if not self.enable_kv_cache_events:
+            return []
+        raise NotImplementedError("Not supported in ElasticBlockPool")
+    
diff --git a/vllm/v1/core/kv_cache_coordinator.py b/vllm/v1/core/kv_cache_coordinator.py
index 5620d9bee..1fb35be05 100644
--- a/vllm/v1/core/kv_cache_coordinator.py
+++ b/vllm/v1/core/kv_cache_coordinator.py
@@ -8,6 +8,7 @@ from vllm.v1.core.single_type_kv_cache_manager import (
     FullAttentionManager, get_manager_for_kv_cache_spec)
 from vllm.v1.kv_cache_interface import FullAttentionSpec, KVCacheConfig
 from vllm.v1.request import Request
+import vllm.envs as envs
 
 
 class KVCacheCoordinator(ABC):
@@ -26,9 +27,37 @@ class KVCacheCoordinator(ABC):
     ):
         self.kv_cache_config = kv_cache_config
         self.max_model_len = max_model_len
-
-        self.block_pool = BlockPool(kv_cache_config.num_blocks, enable_caching,
-                                    enable_kv_cache_events)
+        self.enable_caching = enable_caching
+        
+        if envs.VLLM_USE_KV_SLAB:
+            from vllm.v1.core.block_pool import ElasticBlockPool
+            if self.enable_caching:
+                raise ValueError("Caching is not supported for kv slab")
+            _PIVOT_LAYER_IDX = 0 # MAGIC NUMBER FOR NOW
+            kv_cache_spec = self.kv_cache_config.kv_cache_groups[_PIVOT_LAYER_IDX].kv_cache_spec
+            self.block_size = kv_cache_spec.block_size
+            self.num_gpu_blocks = kv_cache_config.num_blocks
+            cell_size = kv_cache_spec.num_kv_heads * kv_cache_spec.head_size * kv_cache_spec.dtype.itemsize
+
+            num_layers = 0
+            for kv_cache_tensor in self.kv_cache_config.kv_cache_tensors:
+                for _ in kv_cache_tensor.shared_by:
+                    num_layers += 1
+            
+            self.block_pool = ElasticBlockPool(
+                self.num_gpu_blocks,
+                self.block_size,
+                cell_size=cell_size,
+                num_layers=num_layers,
+                enable_caching=enable_caching,
+                engine_id=kv_cache_config.engine_id,
+                local_rank=kv_cache_config.local_rank,
+                host=kv_cache_config.kv_slab_host,
+                ports=kv_cache_config.kv_slab_ports
+            )
+        else:
+            self.block_pool = BlockPool(kv_cache_config.num_blocks, enable_caching,
+                                    enable_kv_cache_events) # TODO enable_kv_cache_events was added!
 
         # Needs special handling for find_longest_cache_hit if eagle is enabled
         self.use_eagle = use_eagle
diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 2e09f4c0a..be5ce43ad 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -15,6 +15,8 @@ from vllm.v1.kv_cache_interface import KVCacheConfig
 from vllm.v1.metrics.stats import PrefixCacheStats
 from vllm.v1.request import Request, RequestStatus
 
+import vllm.envs as envs
+
 logger = init_logger(__name__)
 
 
@@ -77,7 +79,11 @@ class KVCacheManager:
         enable_kv_cache_events: bool = False,
     ) -> None:
         self.max_model_len = max_model_len
-
+        
+        self.use_kv_slab = envs.VLLM_USE_KV_SLAB
+        if self.use_kv_slab:
+            enable_caching = False
+            
         self.enable_caching = enable_caching
         self.caching_hash_fn = sha256 if caching_hash_algo == "sha256" else hash
         self.use_eagle = use_eagle
@@ -255,10 +261,10 @@ class KVCacheManager:
             num_tokens=num_tokens_need_slot,
             new_computed_blocks=new_computed_block_list,
         )
-
-        if num_blocks_to_allocate > self.block_pool.get_num_free_blocks():
-            # Cannot allocate new blocks
-            return None
+        if not envs.VLLM_USE_KV_SLAB:
+            if num_blocks_to_allocate > self.block_pool.get_num_free_blocks():
+                # Cannot allocate new blocks
+                return None
 
         # Touch the computed blocks to make sure they won't be evicted.
         if self.enable_caching:
@@ -275,6 +281,11 @@ class KVCacheManager:
 
         new_blocks = self.coordinator.allocate_new_blocks(
             request.request_id, num_tokens_need_slot)
+        
+        if envs.VLLM_USE_KV_SLAB:
+            for b in new_blocks:
+                if b is None:
+                    return None
 
         # P/D: delay caching blocks if we have to recv from
         # remote. Update state for locally cached blocks.
diff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py
index 6d4bcfe64..845a5e2d9 100644
--- a/vllm/v1/core/kv_cache_utils.py
+++ b/vllm/v1/core/kv_cache_utils.py
@@ -16,7 +16,8 @@ from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,
                                         KVCacheTensor, SlidingWindowSpec)
 from vllm.v1.metrics.stats import PrefixCacheStats
 from vllm.v1.request import Request
-
+import vllm.envs as envs
+import zmq
 logger = init_logger(__name__)
 
 
@@ -708,6 +709,10 @@ def _get_kv_cache_config_uniform_type(vllm_config: VllmConfig,
         kv_cache_tensors=kv_cache_tensors,
         kv_cache_groups=create_kv_cache_group_specs(kv_cache_spec,
                                                     grouped_layer_names),
+        engine_id=None,
+        local_rank=None,
+        kv_slab_host=None,
+        kv_slab_ports=None
     )
 
     num_tokens = num_blocks * vllm_config.cache_config.block_size
@@ -720,6 +725,92 @@ def _get_kv_cache_config_uniform_type(vllm_config: VllmConfig,
                 max_model_len_str, max_concurrency)
     return kv_cache_config
 
+def _get_kv_cache_config_kv_slab(vllm_config: VllmConfig,
+                                      kv_cache_spec: dict[str, KVCacheSpec],
+                                      local_rank: int) -> KVCacheConfig:
+    """
+    FineServe generates the KV cache configuration for a model with one type of KV cache.
+
+    Args:
+        vllm_config: The global VllmConfig
+        kv_cache_spec: The kv cache spec of each attention layer in the model
+
+    Returns:
+        The generated KVCacheConfig
+    """
+    model_name = vllm_config.model_config.model
+    port = vllm_config.cache_config.kv_slab_ports[local_rank]
+    host = vllm_config.cache_config.kv_slab_host
+    
+    context = zmq.Context()
+    socket = context.socket(zmq.REQ)
+    socket.connect(f"tcp://{host}:{port}")
+
+    sync_socket = context.socket(zmq.SUB)
+    sync_socket.connect(f"tcp://{host}:{port+1}")
+    sync_socket.setsockopt_string(zmq.SUBSCRIBE, "")
+
+    # All layers have the same KV cache spec, so we create one kv cache group
+    # for all layers.
+    grouped_layer_names = [list(kv_cache_spec.keys())]
+
+    # (TODO) Support for other qformats
+    _PIVOT_LAYER_IDX = 0
+    num_kv_heads = kv_cache_spec[grouped_layer_names[0][_PIVOT_LAYER_IDX]].num_kv_heads
+    head_size = kv_cache_spec[grouped_layer_names[0][_PIVOT_LAYER_IDX]].head_size
+    dtype = kv_cache_spec[grouped_layer_names[0][_PIVOT_LAYER_IDX]].dtype
+    tokens_per_block = kv_cache_spec[grouped_layer_names[0][_PIVOT_LAYER_IDX]].block_size # tokens per block
+    num_layers = len(grouped_layer_names[0])
+
+    token_size = num_kv_heads * head_size * dtype.itemsize
+    scale_size = 0
+    block_size = tokens_per_block * token_size + scale_size
+    socket.send_pyobj((os.getpid(), "register", (model_name, num_layers, tokens_per_block, token_size, scale_size)))
+    engine_id = socket.recv_pyobj()
+    logger.info(f"[Local Rank {local_rank} Engine {engine_id}] Register num_layers: {num_layers}, block_size: {block_size/1024:.2f}")
+
+    start_signal = sync_socket.recv_string()
+
+    socket.send_pyobj((engine_id, "get_slab_size", None))
+    SLAB_SIZE = socket.recv_pyobj()
+    socket.send_pyobj((engine_id, "get_engine_info", None))
+    engine_info = socket.recv_pyobj()
+    slab_pool_size = engine_info['slab_pool_size']
+    num_slabs = slab_pool_size // num_layers
+    per_layer_size = num_slabs * SLAB_SIZE
+
+    num_blocks_per_slab = SLAB_SIZE // block_size
+    num_blocks = num_slabs * num_blocks_per_slab
+    vllm_config.cache_config.num_gpu_blocks_override = num_blocks
+
+    # Each layer uses a separate Tensor to store its KV cache.
+    kv_cache_tensors = [
+        KVCacheTensor(size=per_layer_size, shared_by=[layer_name])
+        for layer_name in kv_cache_spec
+    ]
+
+    kv_cache_config = KVCacheConfig(
+        num_blocks=num_blocks,
+        kv_cache_tensors=kv_cache_tensors,
+        kv_cache_groups=create_kv_cache_group_specs(kv_cache_spec,
+                                                    grouped_layer_names),
+        local_rank=local_rank,
+        engine_id=engine_id,
+        kv_slab_host=vllm_config.cache_config.kv_slab_host,
+        kv_slab_ports=vllm_config.cache_config.kv_slab_ports
+    )
+    logger.info(f"[Local Rank {local_rank} Engine {engine_id}] num slabs: {num_slabs} num_blocks: {kv_cache_config.num_blocks}")
+
+    num_tokens = num_blocks * vllm_config.cache_config.block_size
+    num_tokens_str = f"{num_tokens:,}"
+    logger.info("GPU KV cache size: %s tokens", num_tokens_str)
+    max_model_len_str = f"{vllm_config.model_config.max_model_len:,}"
+    max_concurrency = get_max_concurrency_for_kv_cache_config(
+        vllm_config, kv_cache_config)
+    logger.info("Maximum concurrency for %s tokens per request: %.2fx",
+                max_model_len_str, max_concurrency)
+
+    return kv_cache_config
 
 def is_kv_cache_page_size_uniform(
         kv_cache_spec: dict[str, KVCacheSpec]) -> bool:
@@ -925,6 +1016,7 @@ def get_kv_cache_config(
     vllm_config: VllmConfig,
     kv_cache_spec: dict[str, KVCacheSpec],
     available_memory: int,
+    local_rank: int
 ) -> KVCacheConfig:
     """
     Generates the KV cache configuration for a model.
@@ -937,25 +1029,28 @@ def get_kv_cache_config(
     Returns:
         The generated KVCacheConfigs
     """
-    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
-
-    if vllm_config.scheduler_config.disable_hybrid_kv_cache_manager:
-        unify_hybrid_kv_cache_specs(kv_cache_spec)
-
-    if is_kv_cache_type_uniform(kv_cache_spec):
-        # KV cache of all layers are the same, which is true for
-        # most models. Allocate the same amount of memory for
-        # each layer.
-        return _get_kv_cache_config_uniform_type(vllm_config, kv_cache_spec,
-                                                 available_memory)
-    elif is_kv_cache_page_size_uniform(kv_cache_spec):
-        # Model contains multiple attention types, but KV cache of all layers
-        # have the same physical memory per block per layer. Split the layers
-        # into groups with the same number of layers, and thus same total page
-        # size.
-        return _get_kv_cache_config_uniform_page_size(vllm_config,
-                                                      kv_cache_spec,
-                                                      available_memory)
+    if envs.VLLM_USE_KV_SLAB:
+        return _get_kv_cache_config_kv_slab(vllm_config, kv_cache_spec, local_rank)
+    else:
+        check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
+
+        if vllm_config.scheduler_config.disable_hybrid_kv_cache_manager:
+            unify_hybrid_kv_cache_specs(kv_cache_spec)
+
+        if is_kv_cache_type_uniform(kv_cache_spec):
+            # KV cache of all layers are the same, which is true for
+            # most models. Allocate the same amount of memory for
+            # each layer.
+            return _get_kv_cache_config_uniform_type(vllm_config, kv_cache_spec,
+                                                    available_memory)
+        elif is_kv_cache_page_size_uniform(kv_cache_spec):
+            # Model contains multiple attention types, but KV cache of all layers
+            # have the same physical memory per block per layer. Split the layers
+            # into groups with the same number of layers, and thus same total page
+            # size.
+            return _get_kv_cache_config_uniform_page_size(vllm_config,
+                                                        kv_cache_spec,
+                                                        available_memory)
 
     raise NotImplementedError
 
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 3d7bbe7e0..4157aa715 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -31,6 +31,7 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+from vllm.v1.core.block_pool import ElasticBlockPool
 
 logger = init_logger(__name__)
 
@@ -156,6 +157,9 @@ class Scheduler(SchedulerInterface):
         )
 
     def schedule(self) -> SchedulerOutput:
+        if isinstance(self.kv_cache_manager.block_pool, ElasticBlockPool):
+            self.kv_cache_manager.block_pool._sync_num_free_blocks()
+
         # NOTE(woosuk) on the scheduling algorithm:
         # There's no "decoding phase" nor "prefill phase" in the scheduler.
         # Each request just has the num_computed_tokens and
@@ -233,17 +237,18 @@ class Scheduler(SchedulerInterface):
                 # allow the lower-priority requests to be scheduled.
                 req_index += 1
                 continue
-
+            
             num_draft_tokens = max(
                 num_new_tokens + request.num_computed_tokens -
                 request.num_tokens, 0)
-
+            
             while True:
                 new_blocks = self.kv_cache_manager.allocate_slots(
                     request,
                     num_new_tokens,
                     num_draft_tokens=num_draft_tokens,
                     num_lookahead_tokens=self.num_lookahead_tokens)
+                
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     # Preempt the lowest-priority request.
@@ -320,7 +325,7 @@ class Scheduler(SchedulerInterface):
             while self.waiting and token_budget > 0:
                 if len(self.running) == self.max_num_running_reqs:
                     break
-
+                
                 request = self.waiting[0]
 
                 # KVTransfer: skip request if still waiting for remote kvs.
@@ -415,7 +420,7 @@ class Scheduler(SchedulerInterface):
                         if num_new_tokens == 0:
                             # The request cannot be scheduled.
                             break
-
+                
                 new_blocks = self.kv_cache_manager.allocate_slots(
                     request,
                     num_new_tokens + num_external_computed_tokens,
@@ -426,6 +431,7 @@ class Scheduler(SchedulerInterface):
                 )
                 if new_blocks is None:
                     # The request cannot be scheduled.
+                #    logger.info("core/scheduler.py: NON NEW BLOCKS BREAK ")
                     break
 
                 # KVTransfer: the connector uses this info to determine
@@ -944,12 +950,20 @@ class Scheduler(SchedulerInterface):
             return None
         prefix_cache_stats = self.kv_cache_manager.make_prefix_cache_stats()
         assert prefix_cache_stats is not None
+        
+        # for logging token usage in FineServe
+        actual_tokens_in_cache = 0
+        
+        for req in self.running:
+            actual_tokens_in_cache += req.num_tokens
+
         return SchedulerStats(
             num_running_reqs=len(self.running),
             num_waiting_reqs=len(self.waiting),
             gpu_cache_usage=self.kv_cache_manager.usage,
             prefix_cache_stats=prefix_cache_stats,
             spec_decoding_stats=spec_decoding_stats,
+            actual_tokens_in_cache=actual_tokens_in_cache
         )
 
     def make_spec_decoding_stats(
@@ -970,6 +984,9 @@ class Scheduler(SchedulerInterface):
     def shutdown(self) -> None:
         if self.kv_event_publisher:
             self.kv_event_publisher.shutdown()
+        
+        if isinstance(self.kv_cache_manager.block_pool, ElasticBlockPool):
+            self.kv_cache_manager.block_pool.shutdown()
 
     ########################################################################
     # KV Connector Related Methods
diff --git a/vllm/v1/core/single_type_kv_cache_manager.py b/vllm/v1/core/single_type_kv_cache_manager.py
index 95222779c..261e916db 100644
--- a/vllm/v1/core/single_type_kv_cache_manager.py
+++ b/vllm/v1/core/single_type_kv_cache_manager.py
@@ -10,7 +10,7 @@ from vllm.v1.core.kv_cache_utils import BlockHash, KVCacheBlock
 from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheSpec,
                                         SlidingWindowSpec)
 from vllm.v1.request import Request
-
+import vllm.envs as envs
 
 class SingleTypeKVCacheManager(ABC):
     """
@@ -124,8 +124,15 @@ class SingleTypeKVCacheManager(ABC):
             return []
         else:
             new_blocks = self.block_pool.get_new_blocks(num_new_blocks)
-            req_blocks.extend(new_blocks)
-            return new_blocks
+            if envs.VLLM_USE_KV_SLAB:
+                if new_blocks is None:
+                    return None
+                else:
+                    req_blocks.extend(new_blocks)
+                    return new_blocks
+            else:
+                req_blocks.extend(new_blocks)
+                return new_blocks
 
     def cache_blocks(self, request: Request, block_hashes: list[BlockHash],
                      num_tokens: int) -> None:
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 7fb36cf59..15ecd0c0b 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import time
 import asyncio
 from collections.abc import AsyncGenerator, Mapping
 from copy import copy
@@ -39,6 +40,7 @@ from vllm.v1.metrics.loggers import (StatLoggerBase, StatLoggerFactory,
                                      setup_default_loggers)
 from vllm.v1.metrics.prometheus import shutdown_prometheus
 from vllm.v1.metrics.stats import IterationStats, SchedulerStats
+from vllm.v1.metrics.loggers import PrometheusStatLogger
 
 logger = init_logger(__name__)
 
@@ -102,6 +104,16 @@ class AsyncLLM(EngineClient):
             custom_stat_loggers=stat_loggers,
         )
 
+        # for FineServe logger
+        self.last_scheduler_stats: Optional[SchedulerStats] = None
+        self.custom_logging_interval = 1
+        self.last_log_time = time.monotonic()
+        self.interval_generated_tokens = 0
+        self.interval_arrived_reqs = 0
+        self.interval_success_reqs = 0
+        self._interval_lock = asyncio.Lock()
+        self._custom_logging_task: Optional[asyncio.Task] = None
+
         # Tokenizer (+ ensure liveness if running in another process).
         self.tokenizer = init_tokenizer_from_configs(
             model_config=vllm_config.model_config,
@@ -202,6 +214,9 @@ class AsyncLLM(EngineClient):
     def shutdown(self):
         """Shutdown, cleaning up the background proc and IPC."""
 
+        if self._custom_logging_task:
+            self._custom_logging_task.cancel()
+
         shutdown_prometheus()
 
         if engine_core := getattr(self, "engine_core", None):
@@ -239,6 +254,10 @@ class AsyncLLM(EngineClient):
             request_id, prompt, params, arrival_time, lora_request,
             tokenization_kwargs, trace_headers, prompt_adapter_request,
             priority, data_parallel_rank)
+        
+        # for FineServe logging arrival rate
+        async with self._interval_lock:
+            self.interval_arrived_reqs += 1
 
         if params.n == 1:
             await self._add_request(request, prompt_str, None, 0, queue)
@@ -364,6 +383,9 @@ class AsyncLLM(EngineClient):
 
         if self.output_handler is not None:
             return
+        
+        if self.log_stats and self._custom_logging_task is None:
+            self._custom_logging_task = asyncio.create_task(self._log_custom_stats_periodically())
 
         # Ensure that the task doesn't have a circular ref back to the AsyncLLM
         # object, or else it won't be garbage collected and cleaned up properly.
@@ -410,12 +432,21 @@ class AsyncLLM(EngineClient):
                     # 4) Logging.
                     # TODO(rob): make into a coroutine and launch it in
                     # background thread once Prometheus overhead is non-trivial.
+                    if iteration_stats:
+                        async with self._interval_lock:
+                            self.interval_generated_tokens += iteration_stats.num_generation_tokens
+                            self.interval_success_reqs += len(iteration_stats.finished_requests)
+
                     if stat_loggers:
                         AsyncLLM._record_stats(
                             stat_loggers[outputs.engine_index],
                             scheduler_stats=outputs.scheduler_stats,
                             iteration_stats=iteration_stats,
                         )
+
+                        if outputs.scheduler_stats:
+                            self.last_scheduler_stats = outputs.scheduler_stats
+
             except Exception as e:
                 logger.exception("AsyncLLM output_handler failed.")
                 output_processor.propagate_error(e)
@@ -431,6 +462,62 @@ class AsyncLLM(EngineClient):
         if self.log_requests:
             logger.info("Aborted request %s.", request_id)
 
+    # FineServe periodic logger
+    async def _log_custom_stats_periodically(self):
+        while True:
+            await asyncio.sleep(self.custom_logging_interval)
+            now = time.monotonic()
+            delta_time = now - self.last_log_time
+            self.last_log_time = now
+
+            async with self._interval_lock:
+                generated_tokens = self.interval_generated_tokens
+                arrived_reqs = self.interval_arrived_reqs
+                success_reqs = self.interval_success_reqs
+
+                self.interval_generated_tokens = 0
+                self.interval_arrived_reqs = 0
+                self.interval_success_reqs = 0
+            
+            tok_per_sec = generated_tokens / delta_time if delta_time > 0 else 0.0
+            arrival_rate = arrived_reqs / delta_time if delta_time > 0 else 0.0
+            req_per_sec = success_reqs / delta_time if delta_time > 0 else 0.0
+
+            stats = self.last_scheduler_stats
+            if stats is None:
+                continue
+
+            try:
+                block_size = self.vllm_config.cache_config.block_size
+                num_total_gpu_blocks = self.vllm_config.cache_config.num_gpu_blocks_override or \
+                                    self.vllm_config.cache_config.num_gpu_blocks
+                
+                block_usage_perc = stats.gpu_cache_usage * 100
+                
+                allocated_blocks = num_total_gpu_blocks * stats.gpu_cache_usage
+                total_token_slots = allocated_blocks * block_size
+                actual_tokens = stats.actual_tokens_in_cache
+                
+                token_usage_perc = (actual_tokens / total_token_slots * 100) if total_token_slots > 0 else 0.0
+
+                log_msg = (
+                    f"[FineServe Stats] "
+                    f"Running: {stats.num_running_reqs}, "
+                    f"Waiting: {stats.num_waiting_reqs}, "
+                    f"Arrival rate: {arrival_rate:.2f}, "
+                    f"Request throughput: {req_per_sec:.2f}, "
+                    f"Generated token throughput: {tok_per_sec:.2f}, "
+                    f"Block Usage: {block_usage_perc:.2f}%, "
+                    f"Token Usage: {token_usage_perc:.2f}%"
+                )
+                logger.info(log_msg)
+                
+            except Exception as e:
+                logger.error(f"Error in custom logging task: {e}", exc_info=True)
+
+
+
+
     @staticmethod
     def _record_stats(
         stat_loggers: list[StatLoggerBase],
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index f36a491a1..0e0bba4f2 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -43,7 +43,7 @@ from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
 from vllm.v1.structured_output import StructuredOutputManager
 from vllm.v1.utils import EngineHandshakeMetadata, EngineZmqAddresses
 from vllm.version import __version__ as VLLM_VERSION
-
+import vllm.envs as envs
 logger = init_logger(__name__)
 
 POLLING_TIMEOUT_S = 2.5
@@ -141,25 +141,30 @@ class EngineCore:
         available_gpu_memory = self.model_executor.determine_available_memory()
 
         assert len(kv_cache_specs) == len(available_gpu_memory)
+        
         # Get the kv cache tensor size
         kv_cache_configs = [
             get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
-                                available_gpu_memory_one_worker)
-            for kv_cache_spec_one_worker, available_gpu_memory_one_worker in
-            zip(kv_cache_specs, available_gpu_memory)
+                                available_gpu_memory_one_worker, local_rank)
+            for local_rank, (kv_cache_spec_one_worker, available_gpu_memory_one_worker) in
+            enumerate(zip(kv_cache_specs, available_gpu_memory))
         ]
 
         # Since we use a shared centralized controller, we need the
         # `kv_cache_config` to be consistent across all workers to make sure
         # all the memory operators can be applied to all workers.
-        unify_kv_cache_configs(kv_cache_configs)
-
-        # All workers have the same kv_cache_config except layer names, so use
-        # an arbitrary one to initialize the scheduler.
-        assert all([
-            cfg.num_blocks == kv_cache_configs[0].num_blocks
-            for cfg in kv_cache_configs
-        ])
+
+        # if different models are loaded on each gpu, 'kv_cache_config' cannot be consistent across all workers
+        # fineserve's kv_slab_manager manages this 
+        if not envs.VLLM_USE_KV_SLAB:
+            unify_kv_cache_configs(kv_cache_configs)
+
+            # All workers have the same kv_cache_config except layer names, so use
+            # an arbitrary one to initialize the scheduler.
+            assert all([
+                cfg.num_blocks == kv_cache_configs[0].num_blocks
+                for cfg in kv_cache_configs
+            ])
         num_gpu_blocks = kv_cache_configs[0].num_blocks
         num_cpu_blocks = 0
         scheduler_kv_cache_config = kv_cache_configs[0]
@@ -555,7 +560,7 @@ class EngineCoreProc(EngineCore):
 
     def _process_engine_step(self) -> bool:
         """Called only when there are unfinished local requests."""
-
+        
         # Step the engine core.
         outputs, model_executed = self.step_fn()
         # Put EngineCoreOutputs into the output queue.
diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py
index 7eff377b7..d1ca7704f 100644
--- a/vllm/v1/engine/core_client.py
+++ b/vllm/v1/engine/core_client.py
@@ -40,7 +40,6 @@ AnyFuture = Union[asyncio.Future[Any], Future[Any]]
 
 _R = TypeVar('_R')  # Return type for collective_rpc
 
-
 class EngineCoreClient(ABC):
     """
     EngineCoreClient: subclasses handle different methods for pushing 
@@ -220,6 +219,17 @@ class InprocClient(EngineCoreClient):
     """
 
     def __init__(self, *args, **kwargs):
+        
+        if len(args) > 0 and isinstance(args[0], VllmConfig):
+            vllm_config = args[0]
+            if (vllm_config.model_config.quantization is not None and 
+                vllm_config.model_config.quantization.lower() == 'qoq'):
+            
+                from vllm.v1.engine.core_qoq import QOQEngineCoreWrapper
+                self.engine_core = QOQEngineCoreWrapper(*args, **kwargs)
+                
+                return
+        
         self.engine_core = EngineCore(*args, **kwargs)
 
     def get_output(self) -> EngineCoreOutputs:
@@ -469,6 +479,14 @@ class MPClient(EngineCoreClient):
         handshake_address = get_engine_client_zmq_addr(
             local_only, host, parallel_config.data_parallel_rpc_port)
 
+        run_engine_core_callback_fcn = EngineCoreProc.run_engine_core
+        
+        if (vllm_config.model_config.quantization is not None and 
+            vllm_config.model_config.quantization.lower() == 'qoq'):
+        
+            from vllm.v1.engine.core_qoq import QOQEngineCoreProc
+            run_engine_core_callback_fcn = QOQEngineCoreProc.run_engine_core
+
         with zmq_socket_ctx(handshake_address, zmq.ROUTER,
                             bind=True) as handshake_socket:
 
@@ -477,7 +495,8 @@ class MPClient(EngineCoreClient):
                 # In server mode, start_index and local_start_index will
                 # both be 0.
                 self.resources.engine_manager = CoreEngineProcManager(
-                    EngineCoreProc.run_engine_core,
+                    #EngineCoreProc.run_engine_core,
+                    run_engine_core_callback_fcn,
                     vllm_config=vllm_config,
                     executor_class=executor_class,
                     log_stats=log_stats,
@@ -620,7 +639,15 @@ class SyncMPClient(MPClient):
         # If an exception arises in process_outputs_socket task,
         # it is forwarded to the outputs_queue so we can raise it
         # from this (run_output_handler) task to shut down the server.
+        
+        if self.outputs_queue.empty():
+            from vllm.v1.metrics.stats import SchedulerStats
+            dummy_outputs = EngineCoreOutputs(
+                scheduler_stats=SchedulerStats()
+            )
+            return dummy_outputs
         outputs = self.outputs_queue.get()
+
         if isinstance(outputs, Exception):
             raise self._format_exception(outputs) from None
         return outputs
diff --git a/vllm/v1/engine/core_qoq.py b/vllm/v1/engine/core_qoq.py
new file mode 100644
index 000000000..b4acb1fa3
--- /dev/null
+++ b/vllm/v1/engine/core_qoq.py
@@ -0,0 +1,976 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+import os
+import queue
+import signal
+import sys
+import threading
+import time
+from collections import deque
+from collections.abc import Generator
+from concurrent.futures import Future
+from contextlib import ExitStack, contextmanager
+from inspect import isclass, signature
+from logging import DEBUG
+from typing import Any, Callable, Optional, TypeVar, Union, Iterable, List
+
+import msgspec
+import zmq
+from collections import defaultdict, deque
+
+from vllm.config import ParallelConfig, VllmConfig
+from vllm.distributed import stateless_destroy_torch_distributed_process_group
+from vllm.executor.multiproc_worker_utils import _add_prefix
+from vllm.logger import init_logger
+from vllm.logging_utils.dump_input import dump_engine_exception
+from vllm.lora.request import LoRARequest
+from vllm.transformers_utils.config import (
+    maybe_register_config_serialize_by_value)
+from vllm.utils import make_zmq_socket, resolve_obj_by_qualname
+from vllm.v1.core.kv_cache_utils import (get_kv_cache_config,
+                                         unify_kv_cache_configs)
+from vllm.v1.core.sched.interface import SchedulerInterface
+from vllm.v1.core.sched.output import SchedulerOutput
+from vllm.v1.core.sched.scheduler import Scheduler as V1Scheduler
+from vllm.v1.engine import (EngineCoreOutput, EngineCoreOutputs, EngineCoreRequest,
+                            EngineCoreRequestType, UtilityOutput)
+from vllm.v1.engine.mm_input_cache import MirroredProcessingCache
+from vllm.v1.executor.abstract import Executor
+from vllm.v1.kv_cache_interface import KVCacheConfig
+from vllm.v1.metrics.stats import SchedulerStats
+from vllm.v1.outputs import ModelRunnerOutput
+from vllm.v1.request import Request, RequestStatus
+from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder
+from vllm.v1.structured_output import StructuredOutputManager
+from vllm.v1.utils import EngineHandshakeMetadata, EngineZmqAddresses
+from vllm.version import __version__ as VLLM_VERSION
+
+logger = init_logger(__name__)
+
+POLLING_TIMEOUT_S = 2.5
+HANDSHAKE_TIMEOUT_MINS = 5
+
+_R = TypeVar('_R')  # Return type for collective_rpc
+
+## Omniserve import
+from omniserve.engine.arg_utils import EngineArgs as QOQEngineArgs
+from omniserve.sampling_params import SamplingParams as QOQSamplingParams
+from omniserve.sequence import (
+    SamplerOutput,
+    Sequence,
+    SequenceGroup,
+    SequenceGroupOutput,
+    SequenceStatus as QOQSequenceStatus,
+)
+from omniserve.engine.llm_engine import LLMEngine as QOQLLMEngine
+##
+
+from vllm.v1.engine import FinishReason
+from vllm.v1.metrics.stats import PrefixCacheStats
+import hashlib
+from dataclasses import dataclass
+
+
+@dataclass
+class QoQConfig:
+    kv_quant_granularity:str = "fine_grained"
+    precision:str = "w4a8kv4"
+    group_size:int = 128
+    ifb_mode:bool = True
+    sparse_decode_mode:int = 0
+    chunk_prefill_size:int = 1024000
+    
+    def compute_hash(self) -> str:
+        factors = []
+        keys = self.__dataclass_fields__.keys()
+        for key in keys:
+            val = getattr(self, key)
+            
+            if val is None: continue
+            factors.append(val)
+            
+        str_factors = str(factors)
+        return hashlib.sha256(str_factors.encode()).hexdigest()
+    
+    @classmethod
+    def override_quantization_method(
+            cls, hf_quant_cfg, user_quant):
+        """
+           Detects if this quantization method can support a given checkpoint
+           format by overriding the user specified quantization method -- 
+           this method should only be overwritten by subclasses in exceptional 
+           circumstances
+        """
+        return None
+
+def get_qoq_engine_args(vllm_config:VllmConfig):
+    engine_args = QOQEngineArgs(vllm_config.model_config.model)
+    
+    # model config
+    vllm_model_config = vllm_config.model_config
+    
+    engine_args.tokenizer = vllm_model_config.tokenizer if vllm_model_config.tokenizer is not None else vllm_config.model_config.model
+    engine_args.tokenizer_mode = vllm_model_config.tokenizer_mode
+    engine_args.trust_remote_code = vllm_model_config.trust_remote_code
+    
+    engine_args.download_dir = vllm_config.load_config.download_dir
+    engine_args.load_format = "auto"
+    engine_args.dtype = vllm_model_config.dtype
+    engine_args.seed = vllm_model_config.seed
+    engine_args.quantization = None 
+    
+    # cache config
+    vllm_cache_confing = vllm_config.cache_config
+    
+    engine_args.gpu_memory_utilization = vllm_cache_confing.gpu_memory_utilization
+    engine_args.swap_space = vllm_cache_confing.swap_space
+    
+    use_kv_slab = bool(int(os.getenv("VLLM_USE_KV_SLAB",'0')))
+
+    if use_kv_slab:
+        engine_args.kv_slab_size = vllm_cache_confing.kv_slab_size
+        engine_args.kv_slab_ports = vllm_cache_confing.kv_slab_ports
+        engine_args.kv_slab_host = vllm_cache_confing.kv_slab_host
+    
+    # scheduler config
+    vllm_scheduler_config = vllm_config.scheduler_config
+    engine_args.max_num_batched_tokens = vllm_scheduler_config.max_num_batched_tokens
+    engine_args.max_num_seqs = vllm_scheduler_config.max_num_seqs    
+    engine_args.max_model_len = vllm_scheduler_config.max_model_len
+    
+    # parallel config
+    vllm_parallel_config = vllm_config.parallel_config
+    engine_args.pipeline_parallel_size = vllm_parallel_config.pipeline_parallel_size
+    engine_args.tensor_parallel_size = vllm_parallel_config.tensor_parallel_size
+    
+    # device & ifb setup
+    ## precision, kv-quant-granularity
+    assert isinstance(vllm_config.additional_config, QoQConfig)
+    qoqconfig = vllm_config.additional_config
+    
+    engine_args.kv_quant_granularity = qoqconfig.kv_quant_granularity
+    engine_args.precision = qoqconfig.precision
+    engine_args.group_size = qoqconfig.group_size
+    engine_args.ifb_mode = qoqconfig.ifb_mode
+    
+    engine_args.sparse_decode_mode = qoqconfig.sparse_decode_mode
+    engine_args.chunk_prefill_size = qoqconfig.chunk_prefill_size
+    
+    ## quant_path
+    engine_args.quant_path = engine_args.model
+
+    return engine_args
+    
+
+class QOQEngineCoreWrapper:
+    """Inner loop of vLLM's Engine."""
+
+    def __init__(self,
+                 vllm_config: VllmConfig,
+                 executor_class: type[Executor],
+                 log_stats: bool,
+                 executor_fail_callback: Optional[Callable] = None):
+        assert vllm_config.model_config.runner_type != "pooling"
+        
+        qengine_args = get_qoq_engine_args(vllm_config)
+        self._engine = QOQLLMEngine.from_engine_args(qengine_args)
+        
+        # plugins need to be loaded at the engine/scheduler level too
+        from vllm.plugins import load_general_plugins
+        load_general_plugins()
+
+        self.vllm_config = vllm_config
+        logger.info("Initializing a V1 LLM engine (v%s) with config: %s",
+                    VLLM_VERSION, vllm_config)
+
+        self.model_executor = None
+
+        vllm_config.cache_config.num_gpu_blocks = self._engine.cache_config.num_retrieval_gpu_blocks
+        vllm_config.cache_config.num_cpu_blocks = self._engine.cache_config.num_retrieval_cpu_blocks
+        self.structured_output_manager = StructuredOutputManager(vllm_config)
+
+        # Setup MM Input Mapper.
+        self.mm_input_cache_server = MirroredProcessingCache(
+            vllm_config.model_config)
+
+        self.finish_reason_mapper = {
+            QOQSequenceStatus.FINISHED_STOPPED: FinishReason.STOP,
+            QOQSequenceStatus.FINISHED_LENGTH_CAPPED: FinishReason.LENGTH,
+            QOQSequenceStatus.FINISHED_ABORTED: FinishReason.ABORT,
+        }
+        
+        self.default_prefix_cache = PrefixCacheStats()
+
+    def add_request(self, request: EngineCoreRequest):
+        """Add request to the scheduler."""
+
+        request_id = request.request_id
+        prompt = ""
+        prompt_token_ids = request.prompt_token_ids
+        
+        sampling_params = request.sampling_params
+        
+        self._engine.add_request(
+            request_id,
+            prompt,
+            sampling_params,
+            prompt_token_ids=prompt_token_ids
+            )
+        
+
+    def abort_requests(self, request_ids: list[str]):
+        """Abort requests from the scheduler."""
+
+        self._engine.abort_request(request_ids)
+
+    def execute_model(self, scheduler_output: SchedulerOutput):
+        try:
+            return self.model_executor.execute_model(scheduler_output)
+        except BaseException as err:
+            # NOTE: This method is exception-free
+            dump_engine_exception(self.vllm_config, scheduler_output,
+                                  self.scheduler.make_stats())
+            # Re-raise exception
+            raise err
+
+    def qoq_process_output(self, output):
+        
+        scheduler_outputs = self._engine.scheduler_outputs
+        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups
+        _outputs: dict[int, list[EngineCoreOutput]] = defaultdict(list)
+        
+        for seq_group, outputs in zip(scheduled_seq_groups, output):
+            self._engine._process_sequence_group_outputs(seq_group, outputs)
+        
+        request_outputs: List = []
+        num_finished = 0
+        
+        for seq_group in scheduled_seq_groups:
+            req_id = seq_group.request_id
+            seqs = seq_group.get_seqs()
+            assert len(seqs) == 1
+
+            client_idx = 0 # TODO
+            if seq_group.is_finished():
+                _outputs[client_idx].append(
+                    EngineCoreOutput(
+                        request_id=req_id,
+                        new_token_ids=[seqs[0].get_last_token_id()],
+                        finish_reason=self.finish_reason_mapper.get(seqs[0].status, None),
+                    )
+                )
+                num_finished += 1
+                self._engine.scheduler.free_seq(seqs[0])
+            else:
+                _outputs[client_idx].append(
+                    EngineCoreOutput(
+                        request_id=req_id,
+                        new_token_ids=[seqs[0].get_last_token_id()],
+                    )
+                )
+                
+        engine_core_outputs = {
+            client_index: EngineCoreOutputs(outputs=outs)
+            for client_index, outs in _outputs.items()
+        }
+        
+        self._engine.scheduler.free_finished_seq_groups()
+
+        return engine_core_outputs
+
+    # JMSON TODO
+    def step(self) -> tuple[dict[int, EngineCoreOutputs], bool]:
+        """Schedule, execute, and make output.
+
+        Returns tuple of outputs and a flag indicating whether the model
+        was executed.
+        """
+
+        output = self._engine.step() # guesss it is qoq SamplerOutput
+        
+        engine_core_outputs = self.qoq_process_output(output)
+        free_blocks = self._engine.scheduler.block_manager.get_retrieval_num_free_gpu_blocks()
+        total_blocks = self._engine.cache_config.num_retrieval_gpu_blocks
+        used_blocks = total_blocks - free_blocks
+        gpu_cache_usage = used_blocks / total_blocks
+        
+        next(iter(engine_core_outputs.values())).scheduler_stats = SchedulerStats(
+                                                                        num_running_reqs=len(self._engine.scheduler.running),
+                                                                        num_waiting_reqs=len(self._engine.scheduler.waiting),
+                                                                        gpu_cache_usage=gpu_cache_usage,
+                                                                        prefix_cache_stats=self.default_prefix_cache,
+                                                                        spec_decoding_stats=None,
+                                                                    )
+        
+        return (engine_core_outputs, 
+                len(self._engine.scheduler.has_unfinished_seqs()) > 0 )
+        
+    def shutdown(self):
+        pass
+
+    def profile(self, is_start: bool = True):
+        pass
+
+    def reset_mm_cache(self):
+        self.mm_input_cache_server.reset()
+
+    def reset_prefix_cache(self):
+        pass
+
+    def sleep(self, level: int = 1):
+        pass
+
+    def wake_up(self, tags: Optional[list[str]] = None):
+        pass
+
+    def is_sleeping(self) -> bool:
+        return False
+
+    def execute_dummy_batch(self):
+        pass
+
+    def add_lora(self, lora_request: LoRARequest) -> bool:
+        raise NotImplementedError
+
+    def remove_lora(self, lora_id: int) -> bool:
+        raise NotImplementedError
+
+    def list_loras(self) -> set[int]:
+        raise NotImplementedError
+
+    def pin_lora(self, lora_id: int) -> bool:
+        raise NotImplementedError
+
+
+    def save_sharded_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        pass
+
+    def collective_rpc(self,
+                       method: Union[str, Callable[..., _R]],
+                       timeout: Optional[float] = None,
+                       args: tuple = (),
+                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:
+        
+        return self._engine._run_workers(method, *args, **kwargs)
+        
+        
+    def save_tensorized_model(
+        self,
+        tensorizer_config,
+    ) -> None:
+        
+        raise NotImplementedError
+
+class QOQEngineCoreProc(QOQEngineCoreWrapper):
+    """ZMQ-wrapper for running EngineCore in background process."""
+
+    ENGINE_CORE_DEAD = b'ENGINE_CORE_DEAD'
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        on_head_node: bool,
+        handshake_address: str,
+        executor_class: type[Executor],
+        log_stats: bool,
+        engine_index: int = 0,
+    ):
+        self.input_queue = queue.Queue[tuple[EngineCoreRequestType, Any]]()
+        self.output_queue = queue.Queue[Union[tuple[int, EngineCoreOutputs],
+                                              bytes]]()
+        executor_fail_callback = lambda: self.input_queue.put_nowait(
+            (EngineCoreRequestType.EXECUTOR_FAILED, b''))
+
+        self.engine_index = engine_index
+        identity = self.engine_index.to_bytes(length=2, byteorder="little")
+        self.engines_running = False
+
+        with self._perform_handshake(handshake_address, identity, on_head_node,
+                                     vllm_config) as addresses:
+            self.client_count = len(addresses.outputs)
+
+            # Set up data parallel environment.
+            self.has_coordinator = addresses.coordinator_output is not None
+            self._init_data_parallel(vllm_config)
+
+            super().__init__(vllm_config, executor_class, log_stats,
+                             executor_fail_callback)
+
+        self.step_fn = self.step
+
+        # Background Threads and Queues for IO. These enable us to
+        # overlap ZMQ socket IO with GPU since they release the GIL,
+        # and to overlap some serialization/deserialization with the
+        # model forward pass.
+        # Threads handle Socket <-> Queues and core_busy_loop uses Queue.
+        threading.Thread(target=self.process_input_sockets,
+                         args=(addresses.inputs, addresses.coordinator_input,
+                               identity),
+                         daemon=True).start()
+        self.output_thread = threading.Thread(
+            target=self.process_output_sockets,
+            args=(addresses.outputs, addresses.coordinator_output,
+                  self.engine_index),
+            daemon=True)
+        self.output_thread.start()
+
+    @contextmanager
+    def _perform_handshake(
+            self, handshake_address: str, identity: bytes, on_head_node: bool,
+            vllm_config: VllmConfig
+    ) -> Generator[EngineZmqAddresses, None, None]:
+        input_ctx = zmq.Context()
+        with make_zmq_socket(input_ctx,
+                             handshake_address,
+                             zmq.DEALER,
+                             identity=identity,
+                             linger=5000,
+                             bind=False) as handshake_socket:
+            # Register engine with front-end.
+            addresses = self.startup_handshake(handshake_socket, on_head_node,
+                                               vllm_config.parallel_config)
+
+            # Update config which may have changed from the handshake
+            vllm_config.__post_init__()
+
+            yield addresses
+
+            # Send ready message.
+            num_gpu_blocks = vllm_config.cache_config.num_gpu_blocks
+            handshake_socket.send(
+                msgspec.msgpack.encode({
+                    "status": "READY",
+                    "local": on_head_node,
+                    "num_gpu_blocks": num_gpu_blocks,
+                }))
+
+    @staticmethod
+    def startup_handshake(
+            handshake_socket: zmq.Socket, on_head_node: bool,
+            parallel_config: ParallelConfig) -> EngineZmqAddresses:
+
+        # Send registration message.
+        handshake_socket.send(
+            msgspec.msgpack.encode({
+                "status": "HELLO",
+                "local": on_head_node,
+            }))
+
+        # Receive initialization message.
+        logger.info("Waiting for init message from front-end.")
+        if not handshake_socket.poll(timeout=HANDSHAKE_TIMEOUT_MINS * 60_000):
+            raise RuntimeError("Did not receive response from front-end "
+                               f"process within {HANDSHAKE_TIMEOUT_MINS} "
+                               f"minutes")
+        init_bytes = handshake_socket.recv()
+        init_message: EngineHandshakeMetadata = msgspec.msgpack.decode(
+            init_bytes, type=EngineHandshakeMetadata)
+        logger.debug("Received init message: %s", init_message)
+
+        received_parallel_config = init_message.parallel_config
+        for key, value in received_parallel_config.items():
+            setattr(parallel_config, key, value)
+
+        return init_message.addresses
+
+    # JMSON TODO
+    @staticmethod
+    def run_engine_core(*args,
+                        dp_rank: int = 0,
+                        local_dp_rank: int = 0,
+                        **kwargs):
+        """Launch EngineCore busy loop in background process."""
+
+        # Signal handler used for graceful termination.
+        # SystemExit exception is only raised once to allow this and worker
+        # processes to terminate without error
+        shutdown_requested = False
+
+        # Ensure we can serialize transformer config after spawning
+        maybe_register_config_serialize_by_value()
+
+        def signal_handler(signum, frame):
+            nonlocal shutdown_requested
+            if not shutdown_requested:
+                shutdown_requested = True
+                raise SystemExit()
+
+        # Either SIGTERM or SIGINT will terminate the engine_core
+        signal.signal(signal.SIGTERM, signal_handler)
+        signal.signal(signal.SIGINT, signal_handler)
+
+        engine_core: Optional[QOQEngineCoreWrapper] = None
+        try:
+            parallel_config: ParallelConfig = kwargs["vllm_config"].parallel_config
+            if parallel_config.data_parallel_size > 1 or dp_rank > 0:
+                # Set data parallel rank for this engine process.
+                
+                # TODO
+                raise NotImplementedError
+            
+                parallel_config.data_parallel_rank = dp_rank
+                parallel_config.data_parallel_rank_local = local_dp_rank
+                engine_core = DPEngineCoreProc(*args, **kwargs)
+            else:
+                engine_core = QOQEngineCoreProc(*args, **kwargs)
+
+            engine_core.run_busy_loop()
+
+        except SystemExit:
+            logger.debug("EngineCore exiting.")
+            raise
+        except Exception as e:
+            if engine_core is None:
+                logger.exception("EngineCore failed to start.")
+            else:
+                logger.exception("EngineCore encountered a fatal error.")
+                engine_core._send_engine_dead()
+            raise e
+        finally:
+            if engine_core is not None:
+                engine_core.shutdown()
+
+    def _init_data_parallel(self, vllm_config: VllmConfig):
+        pass
+
+    def run_busy_loop(self):
+        """Core busy loop of the EngineCore."""
+
+        # Loop until process is sent a SIGINT or SIGTERM
+        while True:
+            # 1) Poll the input queue until there is work to do.
+            self._process_input_queue()
+            # 2) Step the engine core and return the outputs.
+            self._process_engine_step()
+
+    def _process_input_queue(self):
+        """Exits when an engine step needs to be performed."""
+
+        waited = False
+        while not self.engines_running and not self._engine.has_unfinished_requests():
+            if logger.isEnabledFor(DEBUG) and self.input_queue.empty():
+                logger.debug("EngineCore waiting for work.")
+                waited = True
+            req = self.input_queue.get()
+            self._handle_client_request(*req)
+
+        if waited:
+            logger.debug("EngineCore loop active.")
+
+        # Handle any more client requests.
+        while not self.input_queue.empty():
+            req = self.input_queue.get_nowait()
+            self._handle_client_request(*req)
+
+    def _process_engine_step(self) -> bool:
+        """Called only when there are unfinished local requests."""
+
+        # Step the engine core.
+        outputs, model_executed = self.step_fn()
+        
+        # Put EngineCoreOutputs into the output queue.
+        for output in (outputs.items() if outputs else ()):
+            self.output_queue.put_nowait(output)
+
+        return model_executed
+
+    def _handle_client_request(self, request_type: EngineCoreRequestType,
+                               request: Any) -> None:
+        """Dispatch request from client."""
+
+        if request_type == EngineCoreRequestType.ADD:
+            self.add_request(request)
+        elif request_type == EngineCoreRequestType.ABORT:
+            self.abort_requests(request)
+        elif request_type == EngineCoreRequestType.UTILITY:
+            client_idx, call_id, method_name, args = request
+            output = UtilityOutput(call_id)
+            try:
+                method = getattr(self, method_name)
+                output.result = method(
+                    *self._convert_msgspec_args(method, args))
+            except BaseException as e:
+                logger.exception("Invocation of %s method failed", method_name)
+                output.failure_message = (f"Call to {method_name} method"
+                                          f" failed: {str(e)}")
+            self.output_queue.put_nowait(
+                (client_idx, EngineCoreOutputs(utility_output=output)))
+        elif request_type == EngineCoreRequestType.EXECUTOR_FAILED:
+            raise RuntimeError("Executor failed.")
+        else:
+            logger.error("Unrecognized input request type encountered: %s",
+                         request_type)
+
+    @staticmethod
+    def _convert_msgspec_args(method, args):
+        """If a provided arg type doesn't match corresponding target method
+         arg type, try converting to msgspec object."""
+        if not args:
+            return args
+        arg_types = signature(method).parameters.values()
+        assert len(args) <= len(arg_types)
+        return tuple(
+            msgspec.convert(v, type=p.annotation) if isclass(p.annotation)
+            and issubclass(p.annotation, msgspec.Struct)
+            and not isinstance(v, p.annotation) else v
+            for v, p in zip(args, arg_types))
+
+    def _send_engine_dead(self):
+        """Send EngineDead status to the EngineCoreClient."""
+
+        # Put ENGINE_CORE_DEAD in the queue.
+        self.output_queue.put_nowait(QOQEngineCoreProc.ENGINE_CORE_DEAD)
+
+        # Wait until msg sent by the daemon before shutdown.
+        self.output_thread.join(timeout=5.0)
+        if self.output_thread.is_alive():
+            logger.fatal("vLLM shutdown signal from EngineCore failed "
+                         "to send. Please report this issue.")
+
+    def process_input_sockets(self, input_addresses: list[str],
+                              coord_input_address: Optional[str],
+                              identity: bytes):
+        """Input socket IO thread."""
+
+        # Msgpack serialization decoding.
+        add_request_decoder = MsgpackDecoder(EngineCoreRequest)
+        generic_decoder = MsgpackDecoder()
+
+        with ExitStack() as stack, zmq.Context() as ctx:
+            input_sockets = [
+                stack.enter_context(
+                    make_zmq_socket(ctx,
+                                    input_address,
+                                    zmq.DEALER,
+                                    identity=identity,
+                                    bind=False))
+                for input_address in input_addresses
+            ]
+            if coord_input_address is None:
+                coord_socket = None
+            else:
+                coord_socket = stack.enter_context(
+                    make_zmq_socket(ctx,
+                                    coord_input_address,
+                                    zmq.XSUB,
+                                    identity=identity,
+                                    bind=False))
+                # Send subscription message to coordinator.
+                coord_socket.send(b'\x01')
+
+            # Register sockets with poller.
+            poller = zmq.Poller()
+            for input_socket in input_sockets:
+                # Send initial message to each input socket - this is required
+                # before the front-end ROUTER socket can send input messages
+                # back to us.
+                input_socket.send(b'')
+                poller.register(input_socket, zmq.POLLIN)
+            if coord_socket is not None:
+                poller.register(coord_socket, zmq.POLLIN)
+
+            while True:
+                for input_socket, _ in poller.poll():
+                    # (RequestType, RequestData)
+                    type_frame, *data_frames = input_socket.recv_multipart(
+                        copy=False)
+                    request_type = EngineCoreRequestType(
+                        bytes(type_frame.buffer))
+
+                    # Deserialize the request data.
+                    decoder = add_request_decoder if (
+                        request_type
+                        == EngineCoreRequestType.ADD) else generic_decoder
+                    request = decoder.decode(data_frames)
+
+                    # Push to input queue for core busy loop.
+                    self.input_queue.put_nowait((request_type, request))
+
+    def process_output_sockets(self, output_paths: list[str],
+                               coord_output_path: Optional[str],
+                               engine_index: int):
+        """Output socket IO thread."""
+
+        # Msgpack serialization encoding.
+        encoder = MsgpackEncoder()
+        # Send buffers to reuse.
+        reuse_buffers: list[bytearray] = []
+        # Keep references to outputs and buffers until zmq is finished
+        # with them (outputs may contain tensors/np arrays whose
+        # backing buffers were extracted for zero-copy send).
+        pending = deque[tuple[zmq.MessageTracker, Any, bytearray]]()
+
+        # We must set linger to ensure the ENGINE_CORE_DEAD
+        # message is sent prior to closing the socket.
+        with ExitStack() as stack, zmq.Context() as ctx:
+            sockets = [
+                stack.enter_context(
+                    make_zmq_socket(ctx, output_path, zmq.PUSH, linger=4000))
+                for output_path in output_paths
+            ]
+            coord_socket = stack.enter_context(
+                make_zmq_socket(
+                    ctx, coord_output_path, zmq.PUSH, bind=False,
+                    linger=4000)) if coord_output_path is not None else None
+            max_reuse_bufs = len(sockets) + 1
+
+            while True:
+                output = self.output_queue.get()
+                if output == QOQEngineCoreProc.ENGINE_CORE_DEAD:
+                    for socket in sockets:
+                        socket.send(output)
+                    break
+                assert not isinstance(output, bytes)
+                client_index, outputs = output
+                outputs.engine_index = engine_index
+
+                if client_index == -1:
+                    # Don't reuse buffer for coordinator message
+                    # which will be very small.
+                    assert coord_socket is not None
+                    coord_socket.send_multipart(encoder.encode(outputs))
+                    continue
+
+                # Reclaim buffers that zmq is finished with.
+                while pending and pending[-1][0].done:
+                    reuse_buffers.append(pending.pop()[2])
+
+                buffer = reuse_buffers.pop() if reuse_buffers else bytearray()
+                buffers = encoder.encode_into(outputs, buffer)
+                tracker = sockets[client_index].send_multipart(buffers,
+                                                               copy=False,
+                                                               track=True)
+                if not tracker.done:
+                    ref = outputs if len(buffers) > 1 else None
+                    pending.appendleft((tracker, ref, buffer))
+                elif len(reuse_buffers) < max_reuse_bufs:
+                    # Limit the number of buffers to reuse.
+                    reuse_buffers.append(buffer)
+
+
+class DPEngineCoreProc(QOQEngineCoreProc):
+    """ZMQ-wrapper for running EngineCore in background process
+    in a data parallel context."""
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        on_head_node: bool,
+        handshake_address: str,
+        executor_class: type[Executor],
+        log_stats: bool,
+    ):
+
+        self._decorate_logs()
+
+        # Counts forward-passes of the model so that we can synchronize
+        # finished with DP peers every N steps.
+        self.counter = 0
+        self.current_wave = 0
+        self.last_counts = (0, 0)
+
+        # Initialize the engine.
+        dp_rank = vllm_config.parallel_config.data_parallel_rank
+        super().__init__(vllm_config, on_head_node, handshake_address,
+                         executor_class, log_stats, dp_rank)
+
+    def _decorate_logs(self):
+        # Add process-specific prefix to stdout and stderr before
+        # we initialize the engine.
+        from multiprocessing import current_process
+        process_name = current_process().name
+        pid = os.getpid()
+        _add_prefix(sys.stdout, process_name, pid)
+        _add_prefix(sys.stderr, process_name, pid)
+
+    def _init_data_parallel(self, vllm_config: VllmConfig):
+
+        # Configure GPUs and stateless process group for data parallel.
+        dp_rank = vllm_config.parallel_config.data_parallel_rank
+        dp_size = vllm_config.parallel_config.data_parallel_size
+        local_dp_rank = vllm_config.parallel_config.data_parallel_rank_local
+
+        assert dp_size > 1
+        assert 0 <= local_dp_rank <= dp_rank < dp_size
+
+        if vllm_config.kv_transfer_config is not None:
+            # modify the engine_id and append the local_dp_rank to it to ensure
+            # that the kv_transfer_config is unique for each DP rank.
+            vllm_config.kv_transfer_config.engine_id = (
+                f"{vllm_config.kv_transfer_config.engine_id}_dp{local_dp_rank}"
+            )
+            logger.debug("Setting kv_transfer_config.engine_id to %s",
+                         vllm_config.kv_transfer_config.engine_id)
+
+        from vllm.platforms import current_platform
+        device_control_env_var = current_platform.device_control_env_var
+        world_size = vllm_config.parallel_config.world_size
+        os.environ[device_control_env_var] = ",".join(
+            str(current_platform.device_id_to_physical_device_id(i))
+            for i in range(local_dp_rank * world_size, (local_dp_rank + 1) *
+                           world_size))
+
+        self.dp_rank = dp_rank
+        self.dp_group = vllm_config.parallel_config.stateless_init_dp_group()
+
+    def shutdown(self):
+        super().shutdown()
+        if dp_group := getattr(self, "dp_group", None):
+            stateless_destroy_torch_distributed_process_group(dp_group)
+
+    def add_request(self, request: EngineCoreRequest):
+        if self.has_coordinator and request.current_wave != self.current_wave:
+            if request.current_wave > self.current_wave:
+                self.current_wave = request.current_wave
+            elif not self.engines_running:
+                # Request received for an already-completed wave, notify
+                # front-end that we need to start the next one.
+                self.output_queue.put_nowait(
+                    (-1, EngineCoreOutputs(start_wave=self.current_wave)))
+
+        super().add_request(request)
+
+    def _handle_client_request(self, request_type: EngineCoreRequestType,
+                               request: Any) -> None:
+        if request_type == EngineCoreRequestType.START_DP_WAVE:
+            new_wave, exclude_eng_index = request
+            if exclude_eng_index != self.engine_index and (
+                    new_wave >= self.current_wave):
+                self.current_wave = new_wave
+                if not self.engines_running:
+                    logger.debug("EngineCore starting idle loop for wave %d.",
+                                 new_wave)
+                    self.engines_running = True
+        else:
+            super()._handle_client_request(request_type, request)
+
+    def _maybe_publish_request_counts(self):
+        if not self.has_coordinator:
+            return
+
+        # Publish our request counts (if they've changed).
+        counts = self.scheduler.get_request_counts()
+        if counts != self.last_counts:
+            self.last_counts = counts
+            stats = SchedulerStats(*counts)
+            self.output_queue.put_nowait(
+                (-1, EngineCoreOutputs(scheduler_stats=stats)))
+
+    def run_busy_loop(self):
+        """Core busy loop of the EngineCore for data parallel case."""
+
+        # Loop until process is sent a SIGINT or SIGTERM
+        while True:
+            # 1) Poll the input queue until there is work to do.
+            self._process_input_queue()
+
+            # 2) Step the engine core.
+            executed = self._process_engine_step()
+            self._maybe_publish_request_counts()
+
+            local_unfinished_reqs = self.scheduler.has_unfinished_requests()
+            if not executed:
+                if not local_unfinished_reqs and not self.engines_running:
+                    # All engines are idle.
+                    continue
+
+                # We are in a running state and so must execute a dummy pass
+                # if the model didn't execute any ready requests.
+                self.execute_dummy_batch()
+
+            # 3) All-reduce operation to determine global unfinished reqs.
+            self.engines_running = self._has_global_unfinished_reqs(
+                local_unfinished_reqs)
+
+            if not self.engines_running:
+                if self.dp_rank == 0:
+                    # Notify client that we are pausing the loop.
+                    logger.debug("Wave %d finished, pausing engine loop.",
+                                 self.current_wave)
+                    self.output_queue.put_nowait(
+                        (-1,
+                         EngineCoreOutputs(wave_complete=self.current_wave)))
+                self.current_wave += 1
+
+    def _has_global_unfinished_reqs(self, local_unfinished: bool) -> bool:
+
+        # Optimization - only perform finish-sync all-reduce every 24 steps.
+        self.counter += 1
+        if self.counter != 24:
+            return True
+        self.counter = 0
+
+        return ParallelConfig.has_unfinished_dp(self.dp_group,
+                                                local_unfinished)
+
+
+class DPEngineCoreActor(DPEngineCoreProc):
+    """
+    Ray actor for running EngineCore in a data parallel context
+    """
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        on_head_node: bool,
+        addresses: EngineZmqAddresses,
+        executor_class: type[Executor],
+        log_stats: bool,
+        dp_rank: int = 0,
+        local_dp_rank: int = 0,
+    ):
+        self.addresses = addresses
+        vllm_config.parallel_config.data_parallel_rank = dp_rank
+        vllm_config.parallel_config.data_parallel_rank_local = \
+            local_dp_rank
+
+        # Ray sets CUDA_VISIBLE_DEVICES to empty string,
+        # we clean this up to be able to properly initialize
+        # data parallel groups.
+        del os.environ['CUDA_VISIBLE_DEVICES']
+
+        super().__init__(vllm_config, on_head_node, "", executor_class,
+                         log_stats)
+
+    def _decorate_logs(self):
+        pass
+
+    @contextmanager
+    def _perform_handshake(self, handshake_address: str, identity: bytes,
+                           on_head_node: bool, vllm_config: VllmConfig):
+        """
+        For Ray, we don't need to actually perform handshake.
+        All addresses information is known before the actor creation.
+        Therefore, we simply yield these addresses.
+        """
+        yield self.addresses
+
+    def wait_for_init(self):
+        """
+        Wait until the engine core is initialized.
+
+        This is just an empty method. When ray.get() on this method
+        (or any other method of the actor) returns, it is guaranteed
+        that actor creation (i.e., __init__) is complete.
+        """
+        pass
+
+    def run(self):
+        """
+        Run the engine core busy loop.
+        """
+        try:
+            self.run_busy_loop()
+        except SystemExit:
+            logger.debug("EngineCore exiting.")
+            raise
+        except Exception:
+            logger.exception("EngineCore encountered a fatal error.")
+            raise
+        finally:
+            self.shutdown()
diff --git a/vllm/v1/kv_cache_interface.py b/vllm/v1/kv_cache_interface.py
index e938f3bfc..b27f57238 100644
--- a/vllm/v1/kv_cache_interface.py
+++ b/vllm/v1/kv_cache_interface.py
@@ -192,3 +192,11 @@ class KVCacheConfig:
     see `_get_kv_cache_config_uniform_page_size` for more details.
     """
     kv_cache_groups: list[KVCacheGroupSpec]
+
+    """
+    For communication between FineServe ResourceManager
+    """
+    engine_id: Optional[int]
+    local_rank: Optional[int]
+    kv_slab_ports: Optional[list[int]]
+    kv_slab_host: Optional[str]
diff --git a/vllm/v1/metrics/loggers.py b/vllm/v1/metrics/loggers.py
index 2d621ec31..646c15aee 100644
--- a/vllm/v1/metrics/loggers.py
+++ b/vllm/v1/metrics/loggers.py
@@ -206,6 +206,7 @@ class PrometheusStatLogger(StatLoggerBase):
         #
         # Counters
         #
+
         self.counter_num_preempted_reqs = self._counter_cls(
             name="vllm:num_preemptions",
             documentation="Cumulative number of preemption from the engine.",
diff --git a/vllm/v1/metrics/stats.py b/vllm/v1/metrics/stats.py
index 50c8b07fe..88a310f56 100644
--- a/vllm/v1/metrics/stats.py
+++ b/vllm/v1/metrics/stats.py
@@ -40,6 +40,9 @@ class SchedulerStats:
 
     spec_decoding_stats: Optional[SpecDecodingStats] = None
 
+    # for FineServe logging
+    actual_tokens_in_cache: int = 0
+
 
 @dataclass
 class LoRAStats:
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index b1bc727e1..8f25b7c0f 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -289,7 +289,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # means this layer will perform attention using the keys and values
         # from the KV cache of `shared_kv_cache_layers[layer_name]`.
         self.shared_kv_cache_layers: dict[str, str] = {}
-
+        
     def _may_reorder_batch(self, scheduler_output: "SchedulerOutput") -> bool:
         """
         Update the order of requests in the batch based on the attention
@@ -2139,6 +2139,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             dict[str, torch.Tensor]: A map between layer names to their 
             corresponding memory buffer for KV cache.
          """
+        
+        if envs.VLLM_USE_KV_SLAB:
+            return None
+         
         kv_cache_raw_tensors: dict[str, torch.Tensor] = {}
         for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
             tensor = torch.zeros(kv_cache_tensor.size,
@@ -2171,6 +2175,93 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             corresponding memory buffer for KV cache.
         """
         kv_caches: dict[str, torch.Tensor] = {}
+        
+        if envs.VLLM_USE_KV_SLAB:
+            import zmq
+            context = zmq.Context()
+
+            local_rank = kv_cache_config.local_rank
+            port = self.cache_config.kv_slab_ports[local_rank]
+            host = self.cache_config.kv_slab_host
+            self.socket = context.socket(zmq.REQ)
+            self.socket.connect(f"tcp://{host}:{port}") 
+
+            layer_name_size_map = {}
+            for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
+                for lname in kv_cache_tensor.shared_by:
+                    layer_name_size_map[lname] = kv_cache_tensor.size
+                    
+            layer_name_kv_cache_spec_map = {}
+            for kv_cache_group in kv_cache_config.kv_cache_groups:
+                for lname in kv_cache_group.layer_names:
+                    layer_name_kv_cache_spec_map[lname] = kv_cache_group.kv_cache_spec
+                                                    
+            # Obtain representative values
+            _PIVOT_LAYER_IDX = 0 # MAGIC NUMBER FOR NOW            
+            num_layers = 0
+            for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
+                for _ in kv_cache_tensor.shared_by:
+                    num_layers += 1
+            
+            kv_cache_spec = kv_cache_config.kv_cache_groups[_PIVOT_LAYER_IDX].kv_cache_spec
+
+            dtype = kv_cache_spec.dtype
+            num_blocks = kv_cache_config.num_blocks
+            kv_cache_shape = self.attn_backends[
+                    _PIVOT_LAYER_IDX].get_kv_cache_shape(
+                num_blocks, kv_cache_spec.block_size,
+                kv_cache_spec.num_kv_heads, kv_cache_spec.head_size)
+            
+            # get shared kv info
+            self.socket.send_pyobj((kv_cache_config.engine_id, "get_shared_kv", None))
+            shared_kv_info = self.socket.recv_pyobj()
+            shared_kv_info["storage_device"] = local_rank 
+
+            self.socket.send_pyobj((kv_cache_config.engine_id, "get_slab_size", None))
+            SLAB_SIZE = self.socket.recv_pyobj()
+
+            self.socket.send_pyobj((kv_cache_config.engine_id, "get_engine_info", None))
+            engine_slab_pool_info = self.socket.recv_pyobj()
+            slab_pool_size = engine_slab_pool_info['slab_pool_size']
+            shared_kv_slab_offset = engine_slab_pool_info['shared_kv_slab_offset']
+            
+            from torch.multiprocessing.reductions import rebuild_cuda_tensor
+            # rebuild shared kv
+            start_idx = shared_kv_slab_offset * 2 * SLAB_SIZE
+            end_idx = start_idx + slab_pool_size * 2 * SLAB_SIZE
+            shared_kv = rebuild_cuda_tensor(torch.Tensor, **shared_kv_info)
+            shared_kv = shared_kv[start_idx: end_idx]
+            num_slabs = slab_pool_size // num_layers
+
+            shared_kv = shared_kv.view(num_layers, 2, num_slabs * SLAB_SIZE)
+            
+            try:
+                kv_cache_stride_order = self.attn_backends[
+                    _PIVOT_LAYER_IDX].get_kv_cache_stride_order()
+                assert len(kv_cache_stride_order) == len(
+                    kv_cache_shape)
+            except (AttributeError, NotImplementedError):
+                kv_cache_stride_order = tuple(
+                    range(len(kv_cache_shape)))
+
+            kv_cache_backend_shape = tuple(kv_cache_shape[i]
+                                           for i in kv_cache_stride_order)
+            
+            # Maintain original KV shape view.
+            inv_order = [
+                kv_cache_stride_order.index(i)
+                for i in range(len(kv_cache_stride_order))
+            ]
+
+            layer_id = 0            
+            for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
+                for layer_name in kv_cache_tensor.shared_by:
+                    kv_caches[layer_name] = shared_kv[layer_id].view(dtype).view(kv_cache_backend_shape).permute(
+                        *inv_order
+                    )
+                    layer_id += 1
+            return kv_caches
+
         for i, kv_cache_group_spec in enumerate(
                 kv_cache_config.kv_cache_groups):
             kv_cache_spec = kv_cache_group_spec.kv_cache_spec
diff --git a/vllm/v1/worker/gpu_worker.py b/vllm/v1/worker/gpu_worker.py
index b7d244f27..89ab97044 100644
--- a/vllm/v1/worker/gpu_worker.py
+++ b/vllm/v1/worker/gpu_worker.py
@@ -134,18 +134,20 @@ class Worker(WorkerBase):
             # take current memory snapshot
             self.init_snapshot = MemorySnapshot()
             self.requested_memory = (self.init_snapshot.total_memory *
-                                     self.cache_config.gpu_memory_utilization)
-            if self.init_snapshot.free_memory < self.requested_memory:
-                GiB = lambda b: round(b / GiB_bytes, 2)
-                raise ValueError(
-                    f"Free memory on device "
-                    f"({GiB(self.init_snapshot.free_memory)}/"
-                    f"{GiB(self.init_snapshot.total_memory)} GiB) on startup "
-                    f"is less than desired GPU memory utilization "
-                    f"({self.cache_config.gpu_memory_utilization}, "
-                    f"{GiB(self.requested_memory)} GiB). Decrease GPU memory "
-                    f"utilization or reduce GPU memory used by other processes."
-                )
+                                    self.cache_config.gpu_memory_utilization)
+            
+            if not envs.VLLM_USE_KV_SLAB:
+                if self.init_snapshot.free_memory < self.requested_memory:
+                    GiB = lambda b: round(b / GiB_bytes, 2)
+                    raise ValueError(
+                        f"Free memory on device "
+                        f"({GiB(self.init_snapshot.free_memory)}/"
+                        f"{GiB(self.init_snapshot.total_memory)} GiB) on startup "
+                        f"is less than desired GPU memory utilization "
+                        f"({self.cache_config.gpu_memory_utilization}, "
+                        f"{GiB(self.requested_memory)} GiB). Decrease GPU memory "
+                        f"utilization or reduce GPU memory used by other processes."
+                    )
         else:
             raise RuntimeError(
                 f"Not support device type: {self.device_config.device}")
